{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kenoharada/language-model-from-scratch/blob/main/notebooks/Ja/Transformer%20Language%20Model.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training\n",
    "- 学習データの準備\n",
    "- 言語モデルとは\n",
    "- ニューラルネットワークを使用しない手法(uni-gram, bi-gram)\n",
    "- Transformer以前のニューラルネットワークを使用した言語モデル\n",
    "    - MLP\n",
    "    - RNN\n",
    "- Transformerを使用した言語モデル\n",
    "    - Self-AttentionとFeedforward Networkの実装、並列化\n",
    "    - GPT-2の実装\n",
    "\n",
    "プログラミングの解説・エラー解消や用語の解説GPT  \n",
    "https://chatgpt.com/g/g-H1Baw636t-mlxian-bei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install japanize-matplotlib -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの準備\n",
    "ChatGPTのような大規模言語モデルの学習には文書データを大量に集める必要があります。  \n",
    "データの集め方によってモデルの性能が大きく左右されるので、学習データの準備は重要な工程です。以下に参考になりそうな論文を載せておきます。  \n",
    "- [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159)  \n",
    "- [FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)  \n",
    "- [Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance](https://arxiv.org/abs/2403.16952)  \n",
    "- [To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)  \n",
    "- [Instruction Pre-Training: Language Models are Supervised Multitask Learners](https://arxiv.org/abs/2406.14491)  \n",
    "- [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)  \n",
    "- [Physics of Language Models: Part 3.1, Knowledge Storage and Extraction](https://arxiv.org/abs/2309.14316)  \n",
    "- [松尾・岩澤研究室で開催したLLM勉強会での発表資料](https://docs.google.com/presentation/d/14SeP11PcgmNcl93Xt0ziSynxdrOVd2WM/edit?usp=sharing&ouid=101802221278095300433&rtpof=true&sd=true)  \n",
    "\n",
    "今回は[LLM-jpという団体](https://llm-jp.nii.ac.jp/)が整備したコーパスを利用します。\n",
    "- https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/train_9.jsonl.gz\n",
    "# !wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/validation_0.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('train_9.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_text += data['text'] + \"\\n\"\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "print(data.keys())\n",
    "print(data['text'][:100])\n",
    "print(data['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('validation_0.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        val_data_text += data['text'] + \"\\n\"\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break\n",
    "print(data.keys())\n",
    "print(data['text'][:100])\n",
    "print(data['meta'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語モデルとは　　\n",
    "大規模言語モデルの\"言語モデル\"とは、単語列の出現確率をモデル化したものです。  \n",
    "確率を計算できるので、与えられた文章がよく見る文章(確率が高い)なのか、変な文章なのか(確率が低い)を判断することができたり、新たに文章を生成(確率に従ってくじ引きをする)することができます。\n",
    "より良い言語モデルの開発のためには、データとどのように確率をモデル化するかのデザインが重要です。\n",
    "### データについて\n",
    "言語モデルはデータを元に学習するため、そのデータに有益な情報(例えば日本の歴史や法律に関する文章)が含まれていないと、単語自身の理解や単語同士の関係性を学習することができません。  \n",
    "人間が本を読んだり、人との会話を通じて新しい知識を得たり、良い文章の書き方を学んだりするように、言語モデルもデータを通じて学習します。\n",
    "### モデル化について\n",
    "データを得たとしても、どのように確率をモデル化するかのデザインがうまくいかないと、良い言語モデルは作れません。\n",
    "- データ中の文字を数え上げて前の1単語から次の1単語を予測するモデル\n",
    "- ニューラルネットワークの一種であるMLPを用いて、前の数単語から次の1単語を予測するモデル\n",
    "- Transformerを用いて、より長い文脈を考慮して次の単語を予測するモデル  \n",
    "\n",
    "を開発します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークを使用しない、数え上げによる手法(uni-gram, bi-gram)の言語モデル"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram 言語モデル\n",
    "$P\\left(w_1, \\ldots, w_m\\right)=\\prod_{i=1}^{i=m} P\\left(w_i \\mid w_1, \\ldots, w_{i-1}\\right) \\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-n}, \\ldots, w_{i-1}\\right)$\n",
    "\n",
    "#### uni-gramモデル\n",
    "$P\\left(w_1, \\ldots, w_m\\right) \\approx \\prod_{i=1}^{i=m} P(w_i)$\n",
    "\n",
    "#### bi-gramモデル  \n",
    "$P\\left(w_1, \\ldots, w_m\\right)\\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-1}\\right)$\n",
    "\n",
    "$P\\left(w_{i} \\mid w_{i-1}\\right)=\\frac{\\operatorname{count}\\left(w_{i}, w_{i-1}\\right)}{\\operatorname{count}\\left(w_{i}\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mini_train_data_text[:100])\n",
    "print('------------------------')\n",
    "print('全体の文字数: ', len(mini_train_data_text))\n",
    "print('文字の種類: ', len(set(mini_train_data_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_count = {}\n",
    "for character in mini_train_data_text:\n",
    "    character_count[character] = character_count.get(character, 0) + 1\n",
    "# 出現頻度の高い文字を上位5個表示\n",
    "print('出現頻度の高い文字')\n",
    "print(sorted(character_count.items(), key = lambda kv: -kv[1])[:5])\n",
    "\n",
    "# 過去の情報を考慮しないuni-gramモデル\n",
    "print()\n",
    "total_count = sum(character_count.values())\n",
    "ch_0 = '日'\n",
    "ch_1 = '本'\n",
    "ch_2 = '曜'\n",
    "print(f\"'{ch_0}'の確率 = '{ch_0}'の出現回数 ÷ 全体の文字数 =  {character_count[ch_0]} ÷ {total_count} = {character_count[ch_0] / total_count}\")\n",
    "print(f\"'{ch_0 + ch_1}'の確率 = '{ch_0}'の確率 x '{ch_1}'の確率 = {character_count[ch_0] / total_count} x {character_count[ch_1] / total_count} = {character_count[ch_0] * character_count[ch_1] / total_count ** 2}\")\n",
    "print(f\"'{ch_0 + ch_2}'の確率 = '{ch_0}'の確率 x '{ch_2}'の確率 = {character_count[ch_0] / total_count} x {character_count[ch_2] / total_count} = {character_count[ch_0] * character_count[ch_2] / total_count ** 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出現確率の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "total_count = sum(character_count.values())\n",
    "# 上位30文字の出現頻度を取る\n",
    "top_k = 30\n",
    "top_k_key = sorted(character_count.keys(), key = lambda k: -character_count[k])[:top_k]\n",
    "top_k_value = [character_count[k] for k in top_k_key]\n",
    "top_k_probability = [v / total_count for v in top_k_value]\n",
    "\n",
    "# x軸に文字、y軸に出現頻度を取る\n",
    "plt.bar(top_k_key, top_k_probability)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成\n",
    "# greedy decoding\n",
    "max_tokens = 20\n",
    "generated_text = '『勝つか'\n",
    "for _ in range(max_tokens):\n",
    "    next_char = max(character_count, key=lambda k: character_count[k]) # 最も出現頻度が高い文字を選択, greedy decoding\n",
    "    print(repr(next_char))\n",
    "    generated_text += next_char\n",
    "\n",
    "print(repr(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_p sampling\n",
    "import random\n",
    "random.seed(1234)\n",
    "max_tokens = 20\n",
    "top_p = 0.9\n",
    "generated_text = '『勝つか'\n",
    "for _ in range(max_tokens):\n",
    "    sorted_character_count = sorted(character_count.items(), key=lambda kv: -kv[1])\n",
    "    cumulative_probability = 0\n",
    "    vocab = []\n",
    "    vocab_count = []\n",
    "    vocab_total_count = 0\n",
    "    for character, count in sorted_character_count:\n",
    "        cumulative_probability += count / total_count\n",
    "        if cumulative_probability > top_p:\n",
    "            break\n",
    "        vocab.append(character)\n",
    "        vocab_count.append(count)\n",
    "        vocab_total_count += count\n",
    "    probablity = [count / vocab_total_count for count in vocab_count]\n",
    "    next_char = random.choices(vocab, probablity)[0]\n",
    "    print(repr(next_char))\n",
    "    generated_text += next_char\n",
    "print(repr(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Tokenizerの設計も重要です、以下に参考となるリンクを載せておきます。  \n",
    "- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)  \n",
    "- [Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)  \n",
    "- [ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models](https://arxiv.org/abs/2105.13626)  \n",
    "- GPTシリーズのTokenizer: https://github.com/openai/tiktoken\n",
    "- Llama2やLLM-jpのtokenizerを作成する際に利用: https://github.com/google/sentencepiece\n",
    "- Byte Pair Encodingの実装: https://github.com/kenoharada/language-model-from-scratch/blob/main/notebooks/Ja/Tokenizer.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars_in_train_text = sorted(list(set(mini_train_data_text)))\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, chars):\n",
    "        self.str_to_idx = dict()\n",
    "        self.str_to_idx['<|endoftext|>'] = 0\n",
    "        # utf-8\n",
    "        for i in range(256):\n",
    "            if f'<utf8_{i}>' not in self.str_to_idx:\n",
    "                self.str_to_idx[f'<utf8_{i}>'] = len(self.str_to_idx)\n",
    "        for char in chars:\n",
    "            self.str_to_idx[char] = len(self.str_to_idx)\n",
    "        self.idx_to_str = dict()\n",
    "        for key, value in self.str_to_idx.items():\n",
    "            self.idx_to_str[value] = key\n",
    "    \n",
    "    def encode(self, text, eot=False):\n",
    "        result = []\n",
    "        for char in text:\n",
    "            if char not in self.str_to_idx:\n",
    "                utf_8_num = list(char.encode(\"utf-8\"))\n",
    "                for num in utf_8_num:\n",
    "                    result.append(self.str_to_idx[f'<utf8_{num}>'])\n",
    "            else:\n",
    "                result.append(self.str_to_idx[char])\n",
    "        if eot:\n",
    "            result.append(self.str_to_idx['<|endoftext|>'])\n",
    "        return result\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        decoded_with_utf_token = [self.idx_to_str[token] for token in tokens]\n",
    "        decoded_postprocess_utf = []\n",
    "        utf_tokens = []\n",
    "        for token in decoded_with_utf_token:\n",
    "            if token.startswith(\"<utf8_\"):\n",
    "                utf_num = int(token.replace(\"<utf8_\", \"\").replace(\">\", \"\"))\n",
    "                utf_tokens.append(utf_num)\n",
    "            else:\n",
    "                if utf_tokens:\n",
    "                    decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "                    utf_tokens = []\n",
    "                decoded_postprocess_utf.append(token)\n",
    "        if utf_tokens:\n",
    "            decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "            utf_tokens = []\n",
    "        return \"\".join(decoded_postprocess_utf)\n",
    "    \n",
    "    def decode_with_utf(self, tokens):\n",
    "        return \"\".join([self.idx_to_str[token] for token in tokens])\n",
    "\n",
    "tokenizer = Tokenizer(unique_chars_in_train_text) # Tokenizerの初期化、一般的にはByte Pair EncodingやUnigram Language Modelなどを活用してTokenizerを実装する\n",
    "text = '言語モデルの勉強は楽しいです。'\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データにある語\n",
    "'日' in unique_chars_in_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('日'), tokenizer.decode_with_utf([1954]), tokenizer.decode([1954])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データにない未知語\n",
    "'😄' in unique_chars_in_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('😄'), tokenizer.decode_with_utf([241, 160, 153, 133]), tokenizer.decode([241, 160, 153, 133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-gramモデル\n",
    "bigram_count = {}\n",
    "mini_train_data_tokens = []\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "\n",
    "with gzip.open('train_9.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "\n",
    "val_data_tokens = []\n",
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('validation_0.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        val_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break\n",
    "\n",
    "for i in range(len(mini_train_data_tokens) - 1):\n",
    "    bigram = (mini_train_data_tokens[i], mini_train_data_tokens[i+1])\n",
    "    bigram_count[bigram] = bigram_count.get(bigram, 0) + 1\n",
    "\n",
    "# top-kのbigramを表示\n",
    "print('出現頻度の高いbigram')\n",
    "top_k = 10\n",
    "top_k_bigram = sorted(bigram_count.items(), key = lambda kv: -kv[1])[:top_k]\n",
    "print(top_k_bigram)\n",
    "# decodeして確認\n",
    "for bigram, count in top_k_bigram:\n",
    "    print(repr(tokenizer.decode([bigram[0]])), repr(tokenizer.decode([bigram[1]])), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成\n",
    "# greedy decoding\n",
    "max_tokens = 20\n",
    "generated_text = '『勝つか'\n",
    "generated_tokens = tokenizer.encode(generated_text)\n",
    "for _ in range(max_tokens):\n",
    "    next_token = max(bigram_count, key=lambda k: bigram_count[k] if k[0] == generated_tokens[-1] else 0) # 直前の文字が来たときの次の文字の出現頻度が最も高いものを選択\n",
    "    generated_tokens.append(next_token[1])\n",
    "    print(repr(tokenizer.decode([next_token[1]])))\n",
    "print(generated_tokens)\n",
    "print(tokenizer.decode(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "class Ngram:\n",
    "    def __init__(self, n, vocab, laplace=1):\n",
    "        self.n = n\n",
    "        self.vocab = vocab\n",
    "        self.laplace = laplace\n",
    "        self.ngram = defaultdict(lambda: laplace)\n",
    "        self.context_count = defaultdict(lambda: laplace * len(self.vocab))\n",
    "    \n",
    "    def train(self, token_list):\n",
    "        assert isinstance(token_list, list)\n",
    "        for i in range(len(token_list) - self.n + 1):\n",
    "            ngram_list = copy.deepcopy(token_list[i:i+self.n])\n",
    "            ngram_list = [str(i) for i in ngram_list]\n",
    "            context = ngram_list[:-1]\n",
    "            ngram_key = '-'.join(ngram_list)\n",
    "            context_key = '-'.join(context)\n",
    "            self.ngram[ngram_key] += 1\n",
    "            self.context_count[context_key] += 1\n",
    "    \n",
    "\n",
    "    def train_batch(self, token_list):\n",
    "        for tokens in token_list:\n",
    "            self.train(tokens)\n",
    "    \n",
    "    def get_prob(self, ngram):\n",
    "        if self.n == 1:\n",
    "            return self.ngram[ngram] / len(self.vocab)\n",
    "        else:\n",
    "            context = ngram.split('-')[:-1]\n",
    "            context = '-'.join(context)\n",
    "            return self.ngram[ngram] / self.context_count[context]\n",
    "    \n",
    "    def get_prob_distribution(self, n_minus_1_gram):\n",
    "        distribution = []\n",
    "        distribution_dict = {}\n",
    "        for word in self.vocab:\n",
    "            ngram_list = n_minus_1_gram + [word]\n",
    "            ngram = '-'.join([str(i) for i in ngram_list])\n",
    "            # print('hi', ngram)\n",
    "            distribution.append(self.get_prob(ngram))\n",
    "            distribution_dict[word] = self.get_prob(ngram)\n",
    "        return distribution, distribution_dict\n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        if isinstance(token_indexes, torch.Tensor) or isinstance(token_indexes, torch.LongTensor):\n",
    "            token_indexes = token_indexes.tolist()\n",
    "        assert isinstance(token_indexes, list)\n",
    "        batch_size = len(token_indexes)\n",
    "        sequence_length = len(token_indexes[0])\n",
    "        distributions = torch.ones(batch_size, sequence_length, len(self.vocab))\n",
    "        distributions /= len(self.vocab)\n",
    "        for i in range(sequence_length):\n",
    "            for batch in range(batch_size):\n",
    "                if self.n == 2:\n",
    "                    context = [token_indexes[batch][i]]\n",
    "                else:\n",
    "                    if i < self.n - 1:\n",
    "                        if i == 0:\n",
    "                            context = [token_indexes[batch][i]]\n",
    "                        else:\n",
    "                            context = token_indexes[batch][:i+1]\n",
    "                    else:\n",
    "                        context = token_indexes[batch][i-self.n+2:i+1]\n",
    "                distribution, _ = self.get_prob_distribution(context)\n",
    "                distributions[batch, i] = torch.tensor(distribution)\n",
    "        # distributions: (batch_size, sequence_length, vocab_size)\n",
    "        return distributions\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        distributions = self.forward(token_indexes)\n",
    "        distributions = distributions.to(targets.device)\n",
    "        log_distributions = torch.log(distributions)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = log_distributions.shape\n",
    "        loss = F.nll_loss(\n",
    "            log_distributions.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_length=10):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        for _ in range(max_length):\n",
    "            distributions = self.forward(token_indexes)\n",
    "            distributions = distributions[0, -1]\n",
    "            # greedy decoding\n",
    "            next_token = torch.argmax(distributions).item()\n",
    "            token_indexes[0].append(next_token)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "parameters = len(tokenizer.str_to_idx) ** n\n",
    "print(f'パラメータ数: {parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = Ngram(2, tokenizer.str_to_idx.values())\n",
    "ngram.train(mini_train_data_tokens)\n",
    "generated_text = '『勝つか'\n",
    "context_token_indexes = [tokenizer.encode(generated_text)]\n",
    "generated_tokens = ngram.generate(context_token_indexes, max_length=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1:context_length+1]\n",
    "ngram.loss([input_tokens], torch.tensor([target_tokens])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = Ngram(4, tokenizer.str_to_idx.values())\n",
    "ngram.train(mini_train_data_tokens)\n",
    "generated_text = '『勝つか'\n",
    "context_token_indexes = [tokenizer.encode(generated_text)]\n",
    "generated_tokens = ngram.generate(context_token_indexes, max_length=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1:context_length+1]\n",
    "ngram.loss([input_tokens], torch.tensor([target_tokens])).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークを使用した言語モデル　　\n",
    "言語モデルをデザインする際に、過去の文脈を考慮するとより自然な文章のモデリングができると考えられます。  \n",
    "ただ、N-gramモデルのパラメータ数のオーダーは、$O\\left(\\left|V\\right|^n\\right)$　となり、過去の文脈が増えれば増えるほど組み合わせが膨大になります。  \n",
    "組み合わせ単位で数え上げているため、データ中に出現しない組み合わせがあると、その組み合わせの確率は0となり、その後の予測ができなくなってしまいます。  \n",
    "\n",
    "また、組み合わせ別の数え上げでは、単語・文脈をそれぞれ独立したものとして扱っており、単語の意味や文脈を共有した表現が得られません。\n",
    "\n",
    "解決策として組み合わせの表によるモデル化ではなく、ニューラルネットワークと単語ベクトルの表現を用いることでモデル化を行います。\n",
    "\n",
    "ニューラルネットワークの学習の際に行う、次単語予測によって単語ベクトルには単語の意味や概念が付与され、ニューラルネットワークのパラメータには単語同士の関係性が学習されることが期待されます。\n",
    "\n",
    "日本で最も高い山は「？」の？を当てるために、\n",
    "- ？には文法的に名詞が入る\n",
    "- その候補は文脈的に山であり\n",
    "- 日本で最も高いという情報がある、というような文脈上のどの情報に着目するか\n",
    "- 文脈を踏まえた上で今まで得た知識をどのように組み合わせるか\n",
    "\n",
    "ということをニューラルネットワークが学習します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPによるモデル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # 単語ベクトルの取得\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        # FeedForward Layer\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "            )\n",
    "        # vocab_sizeへの変換\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        print('number of parameters:', sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_index: (batch_size, sequence_length)\n",
    "        embedding = self.token_embedding_table(token_indexes)\n",
    "        logits = self.linear(self.ff(embedding))\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length),\n",
    "            reduction='none'\n",
    "            )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            # greedy decoding\n",
    "            next_token = torch.argmax(next_token_probs, dim=-1, keepdim=True)\n",
    "            # next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "d_model = 4\n",
    "d_ff = d_model * 4\n",
    "nn_lm = BigramLanguageModel(vocab_size, d_model, d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '『勝つか'\n",
    "token_indexes = torch.tensor([tokenizer.encode(text)])\n",
    "input_token_indexes = token_indexes[:, :-1]\n",
    "target_token_indexes = token_indexes[:, 1:]\n",
    "print(input_token_indexes)\n",
    "print(tokenizer.decode(input_token_indexes[0].tolist()))\n",
    "\n",
    "print(target_token_indexes)\n",
    "print(tokenizer.decode(target_token_indexes[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = nn_lm.token_embedding_table(input_token_indexes)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_lm.generate(token_indexes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"入力: {input_text} → Token id: {input_token_id} → Token embedding: {token_embeddings} \n",
    "→ Neural Net → \n",
    "{next_token_prediction_prob} <---学習によって近づける---> Target: {target_prob}\"\"\"\n",
    "for idx, token in enumerate(input_token_indexes[0]):\n",
    "    input_text = tokenizer.decode([input_token_indexes[0].tolist()[idx]])\n",
    "    input_token_id = token.item()\n",
    "    token_embeddings = nn_lm.token_embedding_table(token.unsqueeze(0)).squeeze().detach().numpy().tolist()\n",
    "    next_token_prediction = nn_lm(token.unsqueeze(0))\n",
    "    next_token_prediction = F.softmax(next_token_prediction, dim=-1)\n",
    "    next_token_prediction_top_k_index = torch.topk(next_token_prediction, 3).indices\n",
    "\n",
    "    target_token_id = target_token_indexes[0][idx].item()\n",
    "    pickup_token_indexes = [target_token_id] + next_token_prediction_top_k_index[0].tolist()\n",
    "    next_token_prediction_prob = next_token_prediction[0][pickup_token_indexes].tolist()\n",
    "    next_token_prediction_prob_text = [f'{token_id}: {prob:.3f}' for token_id, prob in zip(pickup_token_indexes, next_token_prediction_prob)]\n",
    "    next_token_prediction_prob = ', '.join(next_token_prediction_prob_text)\n",
    "    target_token_distribution = F.one_hot(target_token_indexes[0][idx], num_classes=vocab_size)[pickup_token_indexes].tolist()\n",
    "    target_token_distribution = [f'{token_id}: {prob}' for token_id, prob in zip(pickup_token_indexes, target_token_distribution)]\n",
    "    target_token_distribution = ', '.join(target_token_distribution)\n",
    "    print(template.format(input_text=input_text, input_token_id=input_token_id, token_embeddings=token_embeddings, next_token_prediction_prob=next_token_prediction_prob, target_prob=target_token_distribution))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nn_lm = BigramLanguageModel(vocab_size, d_model, d_ff).to(device)\n",
    "optimizer = torch.optim.AdamW(nn_lm.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 1\n",
    "training_tokens = 0\n",
    "print('before training')\n",
    "val_loss = 0\n",
    "for i in range(0, len(val_data_tokens), batch_size):\n",
    "    batch_tokens = val_data_tokens[i:i+batch_size]\n",
    "    input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "    target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "    val_loss += loss.sum().item()\n",
    "val_loss = val_loss / len(val_data_tokens)\n",
    "print(f'val_loss: {val_loss}')\n",
    "print('start training')\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(mini_train_data_tokens), batch_size):\n",
    "        batch_tokens = mini_train_data_tokens[i:i+batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        loss = nn_lm.loss(input_token_indexes, target_token_indexes)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_tokens += len(batch_tokens)\n",
    "        if training_tokens % 10000 == 0:\n",
    "            print(f'epoch: {epoch}, loss: {loss.item()}, training_tokens: {training_tokens}')\n",
    "    val_loss = 0\n",
    "    for i in range(0, len(val_data_tokens), batch_size):\n",
    "        batch_tokens = val_data_tokens[i:i+batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "        val_loss += loss.sum().item()\n",
    "    val_loss = val_loss / len(val_data_tokens)\n",
    "    print(f'epoch: {epoch}, val_loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '『勝つか'\n",
    "context_token_indexes = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "generated_tokens = nn_lm.generate(context_token_indexes, max_new_tokens=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1:context_length+1]\n",
    "input_token_indexes = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "target_token_indexes = torch.tensor(target_tokens).unsqueeze(0).to(device)\n",
    "nn_lm.loss(input_token_indexes, target_token_indexes).item()\n",
    "\n",
    "# bi-gram(パラメータ数 16516096)の性能: 5.082467079162598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# 当てずっぽうモデルのloss\n",
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "print('vocab_size', vocab_size)\n",
    "print(-math.log(1/len(tokenizer.str_to_idx.values())))\n",
    "\n",
    "math.exp(6.014070987701416), math.exp(2.0) # Understanding Emergent Abilities of Language Models from the Loss Perspective, https://arxiv.org/abs/2403.15796"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習1: MLPの層を増やしたり、学習率などを変更して未来の単語の予測精度向上を試みる、過去のNトークンを入力とするモデルを構築してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE ME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語ベクトルとMLPによるモデル化によって\n",
    "- 密な言語ベクトル表現によって、単語の意味や概念を表現\n",
    "- パラメータ数がO(exp(n))からO(n)へと削減\n",
    "\n",
    "残る課題\n",
    "- 見れる過去の文脈長が固定、増やそうとするとパラメータ数が増加"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNによるモデル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = {'あ': 0, 'い': 1, 'う':2, 'え':3, 'お':4, '<|endoftext|>':5}\n",
    "idx_to_ch = dict((v, k) for (k,v) in vocab.items())\n",
    "text = 'いええい'\n",
    "text = list(text) + ['<|endoftext|>']\n",
    "text = [vocab[ch] for ch in text]\n",
    "model_input = torch.tensor(text[:-1])\n",
    "model_target = torch.tensor(text[1:])\n",
    "print('model input', model_input.tolist(), [idx_to_ch[idx] for idx in model_input.tolist()])\n",
    "print('model target', model_target.tolist(), [idx_to_ch[idx] for idx in model_target.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNの定義\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 5\n",
    "hidden_dim = 3\n",
    "hidden_start = torch.zeros((1, hidden_dim)).T\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "We = torch.randn((hidden_dim, embedding_dim))\n",
    "Wh = torch.randn((hidden_dim, hidden_dim))\n",
    "Wy = torch.randn((vocab_size, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNの順伝播の計算の様子\n",
    "h_t_minus_1 = hidden_start\n",
    "input_history = []\n",
    "\n",
    "# 前のステップの計算が次のステップの計算に影響するため並列化が難しい。\n",
    "# RNNの計算複雑度 len(model_input.tolist()) * hidden_dim * hidden_dim = T * d * d\n",
    "for t in range(len(model_input.tolist())):\n",
    "    idx = model_input.tolist()[t]\n",
    "    input_history.append(idx_to_ch[idx])\n",
    "    print(input_history, '----> RNN ----> ', idx_to_ch[model_target.tolist()[t]])\n",
    "    x = torch.LongTensor([idx])\n",
    "    x = word_embedding_table[x].T\n",
    "    # Attentionを導入したいポイント、過去の文脈がh_t_minus_1に押し込まれる\n",
    "    # ネットワークが単語方向に深くなるため学習が不安定に\n",
    "    # d * d, 系列長によらず一定\n",
    "    h_t = torch.tanh(torch.matmul(We, x) + torch.matmul(Wh, h_t_minus_1))\n",
    "    logits = torch.matmul(Wy, h_t)\n",
    "    print(f'P({idx_to_ch[model_target.tolist()[t]]} | {\", \".join(input_history)}) = {torch.softmax(logits, dim=0).squeeze().tolist()[model_target.tolist()[t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[t]] = 1\n",
    "    h_t_minus_1 = h_t\n",
    "    output_dist = [float('{:.3f}'.format(output)) for output in torch.softmax(logits, dim=0).squeeze().tolist()]\n",
    "    print(f'{output_dist} <-----学習によって近づける-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNによるモデル化によって\n",
    "- 文脈長を固定せずに、任意の長さの文脈を考慮\n",
    "- 文脈長が増えてもパラメータ数が増えない\n",
    "\n",
    "残る課題\n",
    "- 学習が不安定(勾配消失、勾配爆発)\n",
    "- 並列化ができず、学習が遅い\n",
    "- 文脈長が長くなるとトークンの長距離依存関係の把握が難しくなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習2: RNNで学習、生成してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE ME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformerによるモデル化と学習"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention $(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "Attention Is All You Needの式(1)より"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 4\n",
    "d_head = 2\n",
    "\n",
    "K = torch.randn(batch_size, sequence_length, d_head)\n",
    "Q = torch.randn(batch_size, sequence_length, d_head)\n",
    "V = torch.randn(batch_size, sequence_length, d_head)\n",
    "\n",
    "qk_dot_product = Q @ K.transpose(-2, -1)\n",
    "scaled_qk_dot_product = qk_dot_product / (d_head ** 0.5)\n",
    "\n",
    "attention_scores = torch.softmax(scaled_qk_dot_product, dim=-1)\n",
    "attention_output = attention_scores @ V\n",
    "\n",
    "attention_scores_without_scale = torch.softmax(qk_dot_product, dim=-1)\n",
    "attention_output_without_scale = attention_scores_without_scale @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk_dot_product.var(), scaled_qk_dot_product.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores[:, -1, :], attention_scores[:, -1, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores[:, -1, :], attention_scores_without_scale[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\operatorname{FFN}(x)=\\max \\left(0, x W_1+b_1\\right) W_2+b_2$  \n",
    "Attention Is All You Needの式(2)より"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# MLPでの実装の際に実装\n",
    "self.ff = nn.Sequential(\n",
    "    nn.Linear(d_model, d_ff),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(d_ff, d_model)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPT-2 Small\n",
    "vocab_size = 50257\n",
    "d_model = embedding_dim = 768 \n",
    "d_ff = d_model * 4\n",
    "n_head = 12 \n",
    "n_layer = 12\n",
    "d_k = int(d_model / n_head)\n",
    "d_v = int(d_model / n_head)\n",
    "\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "Wq = torch.randn((d_k, embedding_dim))\n",
    "Wk = torch.randn((d_k, embedding_dim))\n",
    "\n",
    "Wv = torch.randn((d_v, embedding_dim))\n",
    "Wo = torch.randn((d_model, d_v * n_head))\n",
    "\n",
    "Wff_1 = torch.randn((d_ff, d_model))\n",
    "Wff_2 = torch.randn((d_model, d_ff))\n",
    "Wy = torch.randn((vocab_size, d_model))\n",
    "\n",
    "params_embedding = vocab_size * embedding_dim\n",
    "print('embddingのパラメータ数', params_embedding)\n",
    "\n",
    "params_positional_embedding = d_model * 1024\n",
    "print('positional embeddingのパラメータ数', params_positional_embedding)\n",
    "\n",
    "params_self_attention = (d_model * d_k * n_head * 2 + d_model * d_v * n_head + d_model * d_v * n_head) * n_layer\n",
    "print('self-attentionのパラメータ数', params_self_attention)\n",
    "\n",
    "params_ff = (d_ff * d_model * 2) * n_layer\n",
    "print('feed-forwardのパラメータ数', params_ff)\n",
    "\n",
    "total_params = params_positional_embedding + params_embedding + params_self_attention + params_ff # + params_unembedding weight tied\n",
    "print('合計パラメータ数', total_params)\n",
    "\n",
    "# パラメータ数の内訳割合\n",
    "print('embeddingの割合', params_embedding / total_params)\n",
    "print('positional embeddingの割合', params_positional_embedding / total_params)\n",
    "print('self-attentionの割合', params_self_attention / total_params)\n",
    "print('feed-forwardの割合', params_ff / total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1層のFFとAttentionのみのモデル\n",
    "vocab_size = len(vocab)\n",
    "d_model = embedding_dim = 3\n",
    "d_ff = d_model * 4\n",
    "n_head = 1 \n",
    "n_layer = 1 \n",
    "d_k = int(d_model / n_head)\n",
    "d_v = int(d_model / n_head)\n",
    "\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "Wq = torch.randn((d_k, embedding_dim))\n",
    "Wk = torch.randn((d_k, embedding_dim))\n",
    "\n",
    "Wv = torch.randn((d_v, embedding_dim))\n",
    "Wo = torch.randn((d_model, d_v * n_head))\n",
    "\n",
    "Wff_1 = torch.randn((d_ff, d_model))\n",
    "Wff_2 = torch.randn((d_model, d_ff))\n",
    "Wy = torch.randn((vocab_size, d_model))\n",
    "\n",
    "params_embedding = vocab_size * embedding_dim\n",
    "print('embddingのパラメータ数', params_embedding)\n",
    "\n",
    "params_positional_embedding = d_model * 1024\n",
    "print('positional embeddingのパラメータ数', params_positional_embedding)\n",
    "\n",
    "params_self_attention = (d_model * d_k * n_head * 2 + d_model * d_v * n_head + d_model * d_v * n_head) * n_layer\n",
    "print('self-attentionのパラメータ数', params_self_attention)\n",
    "\n",
    "params_ff = (d_ff * d_model * 2) * n_layer\n",
    "print('feed-forwardのパラメータ数', params_ff)\n",
    "\n",
    "total_params = params_positional_embedding + params_embedding + params_self_attention + params_ff # + params_unembedding weight tied\n",
    "print('合計パラメータ数', total_params)\n",
    "\n",
    "# パラメータ数の内訳割合\n",
    "print('embeddingの割合', params_embedding / total_params)\n",
    "print('positional embeddingの割合', params_positional_embedding / total_params)\n",
    "print('self-attentionの割合', params_self_attention / total_params)\n",
    "print('feed-forwardの割合', params_ff / total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 並列化されていないSelf-Attention層、FeedForward層の計算\n",
    "quries = []\n",
    "keys = []\n",
    "values = []\n",
    "attention_scores = []\n",
    "attention_outputs = []\n",
    "input_history = []\n",
    "\n",
    "# Self-Attentionの計算複雑度 len(model_input.tolist()) * len(model_input.tolist()) * d_model = T * T * d\n",
    "for t in range(len(model_input.tolist())):\n",
    "    idx = model_input.tolist()[t]\n",
    "    input_history.append(idx_to_ch[idx])\n",
    "    x = torch.LongTensor([idx])\n",
    "    x = word_embedding_table[x].T # + positional_encodings\n",
    "    \n",
    "    # single-head, multi-headになると複数の観点でquery, key, valueの発行をする\n",
    "    query_t = torch.matmul(Wq, x) # 〇〇探してます！ 例: token_0: 自分主語です、目的語とか助詞とか動詞とか探してます！\n",
    "\n",
    "    key_t = torch.matmul(Wk, x) # 〇〇持ってます！ 例: token_0: 自分主語です！ token_1: 自分動詞です!\n",
    "    value_t = torch.matmul(Wv, x) # 中身の詳細です！ 例: token_0: 「拙者」です、珍しい1人称です、お侍さんとかが使ったりします token_1: 「食べる」です、食べ物を口に入れる行為です\n",
    "    \n",
    "    # cross attentionの場合はkey, valueは\n",
    "    # key_t = torch.matmul(Wk, another_modality_x)\n",
    "    # value_t = torch.matmul(Wv, another_modality_x)のようになる\n",
    "\n",
    "    quries.append(query_t)\n",
    "    keys.append(key_t)\n",
    "    values.append(value_t)\n",
    "    # Day2の演習では文章単位でベクトル化したqueryとkeyの内積(類似度)をもとにRetrievalを行った\n",
    "    # 計算複雑度 T * d、系列長によって変化\n",
    "    # 遠く離れた過去の文脈を考慮できる、入力データに応じて相互作用し、その結果が後の深い層でも反映される\n",
    "    attention_score = torch.matmul(query_t.T, torch.stack(keys)) / torch.sqrt(torch.tensor(d_k))\n",
    "    attention_score = torch.softmax(attention_score, dim=0)\n",
    "    attention_scores.append(attention_score.squeeze())\n",
    "    self_attention_output = 0\n",
    "    for i in range(len(attention_score)):\n",
    "        # query、keyのコミュニケーションの結果がvalueに反映される(attention scoreで重み付け)\n",
    "        self_attention_output += attention_score[i] * values[i]\n",
    "    attention_outputs.append(self_attention_output)\n",
    "\n",
    "    # FeedForwardの計算\n",
    "    ff_output = torch.matmul(Wff_2, torch.relu(torch.matmul(Wff_1, self_attention_output)))\n",
    "    # 次単語予測、タスク特化のための分類器を作るような場合には新しくWyを用意して学習する\n",
    "    logits = torch.matmul(Wy, ff_output)\n",
    "    output_dist = [float('{:.3f}'.format(output)) for output in torch.softmax(logits, dim=0).squeeze().tolist()]\n",
    "    \n",
    "    print(f'P({idx_to_ch[model_target.tolist()[t]]} | {\", \".join(input_history)}) = {torch.softmax(logits, dim=0).squeeze().tolist()[model_target.tolist()[t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[t]] = 1\n",
    "    print(f'{output_dist} <-----学習によって近づける-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentionの可視化\n",
    "attention_map = torch.zeros((len(model_input.tolist()), len(model_input.tolist())))\n",
    "for t in range(len(model_input.tolist())):\n",
    "    attention_map[t][:t+1] = attention_scores[t]\n",
    "    print('token', input_history)\n",
    "    print(input_history[t], attention_map[t].tolist())\n",
    "    print('-----' * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未来の情報を用いないようにCausal Attentionを用いて並列化(行列の計算にする)\n",
    "input_ids = model_input.tolist()\n",
    "x = torch.LongTensor([input_ids])\n",
    "x = word_embedding_table[x] # + positional_encodings\n",
    "# print(x.shape) # (1, T, embedding_dim)\n",
    "\n",
    "queries = torch.matmul(x.squeeze(), Wq.T) # (T, d_k)\n",
    "keys = torch.matmul(x.squeeze(), Wk.T) # (T, d_k)\n",
    "values = torch.matmul(x.squeeze(), Wv.T) # (T, d_v)\n",
    "# print(queries.shape, keys.shape, values.shape)\n",
    "\n",
    "attention_scores = torch.matmul(queries, keys.T) / torch.sqrt(torch.tensor(d_k)) # (T, T)\n",
    "# causal attention、未来の情報を用いないようにする\n",
    "attention_mask = torch.tril(torch.ones((len(model_input.tolist()), len(model_input.tolist())))) # (T, T)\n",
    "attention_scores = attention_scores.masked_fill(attention_mask==0, float('-inf'))\n",
    "\n",
    "attention_scores = torch.softmax(attention_scores, dim=1) # (T, T)\n",
    "attention_outputs = torch.matmul(attention_scores, values) # (T, d_v)\n",
    "\n",
    "# FeedForwardの計算\n",
    "ff_output = torch.matmul(torch.relu(torch.matmul(attention_outputs, Wff_1.T)), Wff_2.T) # (T, d_model)\n",
    "# print(ff_output.shape)\n",
    "# 次単語予測\n",
    "logits = torch.matmul(ff_output, Wy.T) # (T, vocab_size)\n",
    "output_dists = torch.softmax(logits, dim=1) # (T, vocab_size)\n",
    "\n",
    "for step_t, output_dist in enumerate(output_dists):\n",
    "    print(f'P({idx_to_ch[model_target.tolist()[step_t]]} | {\", \".join(input_history[:step_t + 1])}) = {output_dist[model_target.tolist()[step_t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[step_t]] = 1\n",
    "    print(f'{[float(\"{:.3f}\".format(output)) for output in output_dist.tolist()]} <-----学習によって近づける-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = torch.matmul(queries, keys.T) / torch.sqrt(torch.tensor(d_k)) # (T, T)\n",
    "print('causal attention前')\n",
    "print(attention_scores)\n",
    "# causal attentionの場合はattention_maskを用いて未来の情報をマスクする\n",
    "attention_mask = torch.tril(torch.ones((len(model_input.tolist()), len(model_input.tolist())))) # (T, T)\n",
    "# 下三角行列で未来の情報をマスク\n",
    "print(attention_mask)\n",
    "# 未来の情報はscoreが0になるように-infを代入(softmaxで0になる)\n",
    "attention_scores = attention_scores.masked_fill(attention_mask==0, float('-inf'))\n",
    "print('causal attention後')\n",
    "print(attention_scores)\n",
    "attention_scores = torch.softmax(attention_scores, dim=1) # (T, T)\n",
    "print('softmax後')\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model, d_head):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.query = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.back_to_d_model = nn.Linear(d_head, d_model)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, d_model)\n",
    "        B, T, d_model = x.size()\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        qk_dot_product = q @ k.transpose(-2, -1) / (d_head ** 0.5)\n",
    "        mask = torch.tril(torch.ones((T, T))).to(qk_dot_product.device)\n",
    "        qk_dot_product = qk_dot_product.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_score = torch.softmax(qk_dot_product, dim=-1)\n",
    "        out = attention_score @ v\n",
    "        out = self.back_to_d_model(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionLM(nn.Module):\n",
    "    def __init__(self, vocab_size, sequence_length, d_model, d_head):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(sequence_length, d_model)\n",
    "        self.head = Head(d_model, d_head)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "        print('number of parameters:', sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        B, T = token_indexes.size()\n",
    "        token_embed = self.embed(token_indexes)\n",
    "        pos_embed = self.pos_embed(torch.arange(T).to(token_embed.device))\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.head(x)\n",
    "        logits = self.unembed(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length),\n",
    "            reduction='none'\n",
    "            )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "d_model = 4\n",
    "d_head = 4\n",
    "sequence_length = 512\n",
    "attention_lm = AttentionLM(vocab_size, sequence_length, d_model, d_head).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_data_tokens = []\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "\n",
    "with gzip.open('train_9.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "\n",
    "val_data_tokens = []\n",
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('validation_0.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        val_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "optimizer = torch.optim.AdamW(attention_lm.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "training_tokens = 0\n",
    "print('before training')\n",
    "val_loss = 0\n",
    "for i in range(0, len(val_data_tokens), sequence_length+1):\n",
    "    batch_tokens = val_data_tokens[i:i+sequence_length+1]\n",
    "    input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "    target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = attention_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "    val_loss += loss.sum().item()\n",
    "val_loss = val_loss / len(val_data_tokens)\n",
    "print(f'val_loss: {val_loss}')\n",
    "print('start training')\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(mini_train_data_tokens), sequence_length+1):\n",
    "        batch_tokens = mini_train_data_tokens[i:i+sequence_length+1]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        loss = attention_lm.loss(input_token_indexes, target_token_indexes)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_tokens += len(batch_tokens)\n",
    "        if training_tokens % ((sequence_length+1)*1000) == 0:\n",
    "            print(f'epoch: {epoch}, loss: {loss.item()}, training_tokens: {training_tokens}')\n",
    "    val_loss = 0\n",
    "    for i in range(0, len(val_data_tokens), sequence_length+1):\n",
    "        batch_tokens = val_data_tokens[i:i+sequence_length+1]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = attention_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "        val_loss += loss.sum().item()\n",
    "    val_loss = val_loss / len(val_data_tokens)\n",
    "    print(f'epoch: {epoch}, val_loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens =  mini_train_data_tokens[:context_length]\n",
    "target_tokens = mini_train_data_tokens[1:context_length+1]\n",
    "input_token_indexes = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "target_token_indexes = torch.tensor(target_tokens).unsqueeze(0).to(device)\n",
    "attention_lm.loss(input_token_indexes, target_token_indexes).item()\n",
    "\n",
    "# bi-gram(パラメータ数 16516096)の性能: 5.082467079162598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '『勝つか'\n",
    "context_token_indexes = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "generated_tokens = attention_lm.generate(context_token_indexes, max_new_tokens=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の実装に加え、学習の安定化のためにResidual ConnectionとLayer Normalizationを追加することでTransformerのblockを実装できます。\n",
    "以下にGPT-2の実装と学習のコードを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2の実装 code from https://github.com/karpathy/makemore/tree/master\n",
    "# https://github.com/karpathy/makemore/blob/master/makemore.py\n",
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Andrej Karpathy\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Transformer Language Model (*exactly* as used in GPT-2)\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual connection\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # residual connection\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"トランスフォーマーのパラメーター数: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習3: Transformerのモデル構造を変更して性能向上を試みる、GPT-2構造で学習させてみる"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- [Andrej Karpathy(元TeslaのAIチームのリーダー、OpenAIの共同創業者)によるGPT-2実装までの講義動画](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
    "- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)  \n",
    "- [CS224N: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "    - https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture08-transformers.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
