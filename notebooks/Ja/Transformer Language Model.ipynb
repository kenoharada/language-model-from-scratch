{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training\n",
    "- 学習データの準備\n",
    "- 言語モデルとは\n",
    "- ニューラルネットワークを使用しない手法(uni-gram, bi-gram)\n",
    "- Transformer以前のニューラルネットワークを使用した言語モデル\n",
    "    - MLP\n",
    "    - RNN\n",
    "- Transformerを使用した言語モデル\n",
    "    - Self-AttentionとFeedforward Networkの実装、並列化\n",
    "    - GPT-2の実装"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの準備\n",
    "ChatGPTのような大規模言語モデルの学習には文書データを大量に集める必要があります。  \n",
    "データの集め方によってモデルの性能が大きく左右されるので、学習データの準備は重要な工程です。以下に参考になりそうな論文を載せておきます。  \n",
    "- [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159)  \n",
    "- [FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)  \n",
    "- [Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance](https://arxiv.org/abs/2403.16952)  \n",
    "- [To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)  \n",
    "- [Instruction Pre-Training: Language Models are Supervised Multitask Learners](https://arxiv.org/abs/2406.14491)  \n",
    "- [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)  \n",
    "- [Physics of Language Models: Part 3.1, Knowledge Storage and Extraction](https://arxiv.org/abs/2309.14316)  \n",
    "- [松尾・岩澤研究室で開催したLLM勉強会での発表資料](https://docs.google.com/presentation/d/14SeP11PcgmNcl93Xt0ziSynxdrOVd2WM/edit?usp=sharing&ouid=101802221278095300433&rtpof=true&sd=true)  \n",
    "\n",
    "今回は[LLM-jpという団体](https://llm-jp.nii.ac.jp/)が整備したコーパスを利用します。\n",
    "- https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-16 18:13:05--  https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/train_9.jsonl.gz\n",
      "Resolving gitlab.llm-jp.nii.ac.jp (gitlab.llm-jp.nii.ac.jp)... 157.1.137.152\n",
      "Connecting to gitlab.llm-jp.nii.ac.jp (gitlab.llm-jp.nii.ac.jp)|157.1.137.152|:443... connected.\n",
      "WARNING: cannot verify gitlab.llm-jp.nii.ac.jp's certificate, issued by ‘CN=NII Open Domain CA - G7 RSA,O=SECOM Trust Systems CO.\\\\,LTD.,C=JP’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 137011103 (131M) [application/gzip]\n",
      "Saving to: ‘train_9.jsonl.gz’\n",
      "\n",
      "train_9.jsonl.gz    100%[===================>] 130.66M  76.8MB/s    in 1.7s    \n",
      "\n",
      "2024-09-16 18:13:07 (76.8 MB/s) - ‘train_9.jsonl.gz’ saved [137011103/137011103]\n",
      "\n",
      "--2024-09-16 18:13:07--  https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/validation_0.jsonl.gz\n",
      "Resolving gitlab.llm-jp.nii.ac.jp (gitlab.llm-jp.nii.ac.jp)... 157.1.137.152\n",
      "Connecting to gitlab.llm-jp.nii.ac.jp (gitlab.llm-jp.nii.ac.jp)|157.1.137.152|:443... connected.\n",
      "WARNING: cannot verify gitlab.llm-jp.nii.ac.jp's certificate, issued by ‘CN=NII Open Domain CA - G7 RSA,O=SECOM Trust Systems CO.\\\\,LTD.,C=JP’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1657379 (1.6M) [application/gzip]\n",
      "Saving to: ‘validation_0.jsonl.gz’\n",
      "\n",
      "validation_0.jsonl. 100%[===================>]   1.58M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-09-16 18:13:08 (20.9 MB/s) - ‘validation_0.jsonl.gz’ saved [1657379/1657379]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/train_9.jsonl.gz\n",
    "# !wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/validation_0.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'meta'])\n",
      "チリコシーは、アメリカ合衆国オハイオ州中央部南ロス郡の都市であり、同郡の郡庁所在地である。コロンバス大都市圏に属している。\n",
      "\n",
      "2010年の国勢調査では人口21,901 人だった。ロス郡では唯一の都市で\n",
      "{'id': '2973866', 'title': 'チリコシー (オハイオ州)', 'url': 'https://ja.wikipedia.org/wiki/%E3%83%81%E3%83%AA%E3%82%B3%E3%82%B7%E3%83%BC%20%28%E3%82%AA%E3%83%8F%E3%82%A4%E3%82%AA%E5%B7%9E%29'}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('train_9.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 各行をJSONとして読み込む\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_text += data['text'] + \"\\n\"\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "print(data.keys())\n",
    "print(data['text'][:100])\n",
    "print(data['meta'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語モデルとは　　\n",
    "大規模言語モデルの\"言語モデル\"とは、単語列の出現確率をモデル化したものです。  \n",
    "確率を計算できるので、与えられた文章がよく見る文章(確率が高い)なのか、変な文章なのか(確率が低い)を判断することができたり、新たに文章を生成(確率に従ってくじ引きをする)することができます。\n",
    "より良い言語モデルの開発のためには、データとどのように確率をモデル化するかのデザインが重要です。\n",
    "### データについて\n",
    "言語モデルはデータを元に学習するため、そのデータに有益な情報(例えば日本の歴史や法律に関する文章)が含まれていないと、単語自身の理解や単語同士の関係性を学習することができません。  \n",
    "人間が本を読んだり、人との会話を通じて新しい知識を得たり、良い文章の書き方を学んだりするように、言語モデルもデータを通じて学習します。\n",
    "### モデル化について\n",
    "データを得たとしても、どのように確率をモデル化するかのデザインがうまくいかないと、良い言語モデルは作れません。\n",
    "- データ中の文字を数え上げて前の1単語から次の1単語を予測するモデル\n",
    "- ニューラルネットワークの一種であるMLPを用いて、前の数単語から次の1単語を予測するモデル\n",
    "- Transformerを用いて、より長い文脈を考慮して次の単語を予測するモデル  \n",
    "\n",
    "を開発します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークを使用しない、数え上げによる手法(uni-gram, bi-gram)の言語モデル"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram 言語モデル\n",
    "$P\\left(w_1, \\ldots, w_m\\right)=\\prod_{i=1}^{i=m} P\\left(w_i \\mid w_1, \\ldots, w_{i-1}\\right) \\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-n}, \\ldots, w_{i-1}\\right)$\n",
    "\n",
    "#### uni-gramモデル\n",
    "$P\\left(w_1, \\ldots, w_m\\right) \\approx \\prod_{i=1}^{i=m} P(w_i)$\n",
    "\n",
    "#### bi-gramモデル  \n",
    "$P\\left(w_1, \\ldots, w_m\\right)\\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-1}\\right)$\n",
    "\n",
    "$P\\left(w_{i} \\mid w_{i-1}\\right)=\\frac{\\operatorname{count}\\left(w_{i}, w_{i-1}\\right)}{\\operatorname{count}\\left(w_{i}\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "『勝つか死ぬか』はHBO(日本ではスター・チャンネルが放送)のファンタジー・ドラマ・シリーズである『ゲーム・オブ・スローンズ』の第1章『七王国戦記』の第7話である。プロデューサーでもあるデイヴィッド・\n",
      "------------------------\n",
      "全体の文字数:  1569582\n",
      "文字の種類:  3807\n"
     ]
    }
   ],
   "source": [
    "print(mini_train_data_text[:100])\n",
    "print('------------------------')\n",
    "print('全体の文字数: ', len(mini_train_data_text))\n",
    "print('文字の種類: ', len(set(mini_train_data_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出現頻度の高い文字\n",
      "[(' ', 53441), ('\\n', 44189), ('の', 38675), ('、', 30453), ('ー', 29588)]\n",
      "\n",
      "'日'の確率 = '日'の出現回数 ÷ 全体の文字数 =  7361 ÷ 1569582 = 0.004689783649404746\n",
      "'日本'の確率 = '日'の確率 x '本'の確率 = 0.004689783649404746 x 0.001984604818352912 = 9.307367227641361e-06\n",
      "'日曜'の確率 = '日'の確率 x '曜'の確率 = 0.004689783649404746 x 0.00018093989355127672 = 8.485689543018127e-07\n"
     ]
    }
   ],
   "source": [
    "character_count = {}\n",
    "for character in mini_train_data_text:\n",
    "    character_count[character] = character_count.get(character, 0) + 1\n",
    "# 出現頻度の高い文字を上位5個表示\n",
    "print('出現頻度の高い文字')\n",
    "print(sorted(character_count.items(), key = lambda kv: -kv[1])[:5])\n",
    "\n",
    "# 過去の情報を考慮しないuni-gramモデル\n",
    "print()\n",
    "total_count = sum(character_count.values())\n",
    "ch_0 = '日'\n",
    "ch_1 = '本'\n",
    "ch_2 = '曜'\n",
    "print(f\"'{ch_0}'の確率 = '{ch_0}'の出現回数 ÷ 全体の文字数 =  {character_count[ch_0]} ÷ {total_count} = {character_count[ch_0] / total_count}\")\n",
    "print(f\"'{ch_0 + ch_1}'の確率 = '{ch_0}'の確率 x '{ch_1}'の確率 = {character_count[ch_0] / total_count} x {character_count[ch_1] / total_count} = {character_count[ch_0] * character_count[ch_1] / total_count ** 2}\")\n",
    "print(f\"'{ch_0 + ch_2}'の確率 = '{ch_0}'の確率 x '{ch_2}'の確率 = {character_count[ch_0] / total_count} x {character_count[ch_2] / total_count} = {character_count[ch_0] * character_count[ch_2] / total_count ** 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Tokenizerの設計も重要です、以下に参考となるリンクを載せておきます。  \n",
    "- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)  \n",
    "- [Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)  \n",
    "- [ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models](https://arxiv.org/abs/2105.13626)  \n",
    "- GPTシリーズのTokenizer: https://github.com/openai/tiktoken\n",
    "- Llama2やLLM-jpのtokenizerを作成する際に利用: https://github.com/google/sentencepiece\n",
    "- Byte Pair Encodingの実装: https://github.com/kenoharada/language-model-from-scratch/blob/main/notebooks/Ja/Tokenizer.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3054, 3098, 438, 411, 447, 335, 771, 1365, 336, 1900, 312, 293, 328, 314, 270, 0]\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(mini_train_data_text)))\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, chars):\n",
    "        self.str_to_idx = {s:i+1 for i,s in enumerate(chars)}\n",
    "        self.str_to_idx['<|endoftext|>'] = 0\n",
    "        self.idx_to_str = {i:s for s,i in self.str_to_idx.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        chs = list(text) + ['<|endoftext|>']\n",
    "        return [self.str_to_idx[ch] for ch in chs]\n",
    "    \n",
    "    def decode(self, index_list):\n",
    "        return ''.join([self.idx_to_str[i] for i in index_list])\n",
    "\n",
    "tokenizer = Tokenizer(chars) # Tokenizerの初期化、一般的にはByte Pair EncodingやUnigram Language Modelなどを活用してTokenizerを実装する\n",
    "text = '言語モデルの勉強は楽しいです。'\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "言語モデルの勉強は楽しいです。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N[2:]).float()\n",
    "P /= P.sum(0, keepdims=True)\n",
    "g = torch.Generator().manual_seed(1)\n",
    "context = '吾輩'\n",
    "out = list(context)\n",
    "print('入力: ', ''.join(out))\n",
    "while True:\n",
    "    idx = torch.multinomial(P, num_samples=1, replacement=True, generator=g).item() + 2\n",
    "    print(idx, idx_to_str[idx])\n",
    "    out.append(idx_to_str[idx])\n",
    "    if len(out) >= 10:\n",
    "        break\n",
    "print('生成結果: ', ''.join(out[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count = {}\n",
    "for sentence in sentences:\n",
    "    chs = ['<S>'] + list(sentence) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        bigram_count[bigram] = bigram_count.get(bigram, 0) + 1\n",
    "print('出現頻度の高い、2文字のペア')\n",
    "print(sorted(bigram_count.items(), key = lambda kv: -kv[1])[:5])\n",
    "\n",
    "import torch\n",
    "N = torch.zeros((len(set(all_text_data))+2, len(set(all_text_data))+2), dtype=torch.int32)\n",
    "chars = sorted(list(set(all_text_data)))\n",
    "str_to_idx = {s:i+2 for i,s in enumerate(chars)}\n",
    "str_to_idx['<S>'] = 0\n",
    "str_to_idx['<E>'] = 1\n",
    "idx_to_str = {i:s for s,i in str_to_idx.items()}\n",
    "\n",
    "for sentence in sentences:\n",
    "    chs = ['<S>'] + list(sentence) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = str_to_idx[ch1]\n",
    "        idx2 = str_to_idx[ch2]\n",
    "        N[idx1, idx2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count[('は', '猫')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1文字前を考慮するbi-gramモデル\n",
    "print(f\"'は→猫'('は'の後に'猫'が続く)の確率 = 'は猫'の出現回数 ÷ 'は'の出現回数 =  {N[str_to_idx['は'], str_to_idx['猫']]} ÷ {N.sum(1)[str_to_idx['は']]} = {N[str_to_idx['は'], str_to_idx['猫']] / N.sum(1)[str_to_idx['は']]}\")\n",
    "print(f\"'吾輩は猫'の確率 = '吾'の確率 x '吾→輩'の確率 x '輩→は'の確率 x 'は→猫'の確率 = {word_count['吾'] / total_count} x {N[str_to_idx['吾'], str_to_idx['輩']] / N.sum(1)[str_to_idx['吾']]} x {N[str_to_idx['輩'], str_to_idx['は']] / N.sum(1)[str_to_idx['輩']]} x {N[str_to_idx['は'], str_to_idx['猫']] / N.sum(1)[str_to_idx['は']]} = {(word_count['吾'] / total_count) * (N[str_to_idx['吾'], str_to_idx['輩']] / N.sum(1)[str_to_idx['吾']]) * (N[str_to_idx['輩'], str_to_idx['は']] / N.sum(1)[str_to_idx['輩']]) * (N[str_to_idx['は'], str_to_idx['猫']] / N.sum(1)[str_to_idx['は']])}\")\n",
    "print(f\"'吾輩は犬'の確率 = '吾'の確率 x '吾→輩'の確率 x '輩→は'の確率 x 'は→犬'の確率 = {word_count['吾'] / total_count} x {N[str_to_idx['吾'], str_to_idx['輩']] / N.sum(1)[str_to_idx['吾']]} x {N[str_to_idx['輩'], str_to_idx['は']] / N.sum(1)[str_to_idx['輩']]} x {N[str_to_idx['は'], str_to_idx['犬']] / N.sum(1)[str_to_idx['は']]} = {(word_count['吾'] / total_count) * (N[str_to_idx['吾'], str_to_idx['輩']] / N.sum(1)[str_to_idx['吾']]) * (N[str_to_idx['輩'], str_to_idx['は']] / N.sum(1)[str_to_idx['輩']]) * (N[str_to_idx['は'], str_to_idx['犬']] / N.sum(1)[str_to_idx['は']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1文字前を考慮するbi-gramモデルによる生成\n",
    "P = (N).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "context = '吾輩'\n",
    "out = list(context)\n",
    "print('入力: ', ''.join(out))\n",
    "idx = str_to_idx[out[-1]]\n",
    "while True:\n",
    "    p = P[idx]\n",
    "    idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(idx_to_str[idx])\n",
    "    if idx == 1 or len(out) >= 100:\n",
    "        break\n",
    "print('生成結果: ', ''.join(out[:-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークを使用した言語モデル　　\n",
    "言語モデルをデザインする際に、過去の文脈を考慮するとより自然な文章のモデリングができると考えられます。  \n",
    "ただ、N-gramモデルのパラメータ数のオーダーは、$O\\left(\\left|V\\right|^n\\right)$　となり、過去の文脈が増えれば増えるほど組み合わせが膨大になります。  \n",
    "過去の3文字を考慮するだけでも、今回の文字種類数だとパラメータ数(表のマス目の数) 3016 ** 4  = 82,741,873,217,536(約80兆)となり、今回の小説データの文字数(337,202)より多くなり、GPT-3のパラメータ数1750億を超えます。  \n",
    "組み合わせ単位で数え上げているため、データ中に出現しない組み合わせがあると、その組み合わせの確率は0となり、その後の予測ができなくなってしまいます。  \n",
    "\n",
    "また、組み合わせ別の数え上げでは、単語・文脈をそれぞれ独立したものとして扱っており、単語の意味や文脈を共有した表現が得られません。\n",
    "\n",
    "解決策として組み合わせの表によるモデル化ではなく、ニューラルネットワークと単語ベクトルの表現を用いることでモデル化を行います。\n",
    "\n",
    "ニューラルネットワークの学習の際に行う、次単語予測によって単語ベクトルには単語の意味や概念が付与され、ニューラルネットワークのパラメータには単語同士の関係性が学習されることが期待されます。\n",
    "\n",
    "日本で最も高い山は「？」の？を当てるために、\n",
    "- ？には文法的に名詞が入る\n",
    "- その候補は文脈的に山であり\n",
    "- 日本で最も高いという情報がある、というような文脈上のどの情報に着目するか\n",
    "- 文脈を踏まえた上で今まで得た知識をどのように組み合わせるか\n",
    "\n",
    "ということをニューラルネットワークが学習します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPによるモデル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPの学習のためのデータの準備\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "sentences = clean_text('neco.html')\n",
    "chars = sorted(list(set(''.join(sentences))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['<E>'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# build the dataset\n",
    "window_size = 4\n",
    "\n",
    "def build_dataset(sentences, window_size):  \n",
    "  X, Y, total_words = [], [], []\n",
    "  for w in sentences:\n",
    "    context = [0] * window_size\n",
    "    for ch in w:\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      total_words.append(w)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "    ix = stoi['<E>']\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    total_words.append(w + '<E>')\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  return X, Y, total_words\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "n1 = int(0.99*len(sentences))\n",
    "n2 = int(0.995*len(sentences))\n",
    "\n",
    "Xtr, Ytr, sentencetr = build_dataset(sentences[:n1], window_size)\n",
    "Xdev, Ydev, _ = build_dataset(sentences[n1:n2], window_size)\n",
    "Xte, Yte, _ = build_dataset(sentences[n2:], window_size)\n",
    "\n",
    "sample_idx = 4\n",
    "example_num = 3\n",
    "embedding_dim = 3\n",
    "embedding_table = torch.randn((len(stoi), embedding_dim))\n",
    "\n",
    "print(sentencetr[sample_idx])\n",
    "for n in range(example_num):\n",
    "    print([itos[idx] for idx in Xtr[sample_idx + n].tolist()], '---MLP--->', itos[Ytr[sample_idx + n].item()])\n",
    "    print(Xtr[sample_idx + n].tolist(), '---MLP--->', Ytr[sample_idx + n].item())\n",
    "    for idx, embedding in enumerate(embedding_table[Xtr[sample_idx + n].tolist()].tolist()):\n",
    "        if idx == int(window_size / 2):\n",
    "            print(embedding, '  ----------MLP---------->', Ytr[sample_idx + n].item())\n",
    "        else:\n",
    "            print(embedding)\n",
    "    print('--'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPの定義\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, window_size, vocab_size, n_embd, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.window_size * n_embd, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, self.vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, idx):\n",
    "        embs = self.wte(idx) # (b, window_size, n_embd)\n",
    "        batch_size, window_size, n_embd = embs.shape\n",
    "        x = embs.view(batch_size, -1) # (b, window_size, n_embd * window_size)\n",
    "        logits = self.mlp(x)\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習1: MLPの層を増やしたり、学習率などを変更して未来の単語の予測精度向上を試みる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "total_step_num = 10\n",
    "window_size = 4\n",
    "embedding_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "mlp = MLP(window_size, len(stoi), embedding_dim, hidden_dim)\n",
    "mlp.train()\n",
    "optimizer = torch.optim.AdamW(mlp.parameters(), lr=5e-4, weight_decay=0.01, betas=(0.9, 0.99), eps=1e-8)\n",
    "lossi = []\n",
    "stepi = []\n",
    "for step in range(total_step_num):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    logits = mlp(Xtr[ix])\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    stepi.append(step)\n",
    "    lossi.append(loss.log10().item())\n",
    "# 最終的なloss\n",
    "print(loss.log10().item())\n",
    "# lossの推移\n",
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP言語モデルによる生成\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "out = []\n",
    "context_str = '吾輩（わ'\n",
    "context_idx = [stoi[ch] for ch in context_str]\n",
    "print('入力: ', context_str)\n",
    "while True:\n",
    "    x = torch.tensor(context_idx).unsqueeze(0)\n",
    "    logits = mlp(x)\n",
    "    ix = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1, replacement=True, generator=g).item()\n",
    "    context_idx = context_idx[1:] + [ix]\n",
    "    if ix == 0 or len(out) >= 100:\n",
    "        break\n",
    "    out.append(ix)\n",
    "print('生成結果: ', context_str + ''.join(itos[i] for i in out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語ベクトルとMLPによるモデル化によって\n",
    "- 密な言語ベクトル表現によって、単語の意味や概念を表現\n",
    "- パラメータ数がO(exp(n))からO(n)へと削減\n",
    "\n",
    "残る課題\n",
    "- 見れる過去の文脈長が固定、増やそうとするとパラメータ数が増加"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNによるモデル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = {'あ': 0, 'い': 1, 'う':2, 'え':3, 'お':4, '<sos>':5, '<eos>':6}\n",
    "idx_to_ch = dict((v, k) for (k,v) in vocab.items())\n",
    "text = 'いえい'\n",
    "text = ['<sos>'] + list(text) + ['<eos>']\n",
    "text = [vocab[ch] for ch in text]\n",
    "model_input = torch.tensor(text[:-1])\n",
    "model_target = torch.tensor(text[1:])\n",
    "print('model input', model_input.tolist(), [idx_to_ch[idx] for idx in model_input.tolist()])\n",
    "print('model target', model_target.tolist(), [idx_to_ch[idx] for idx in model_target.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNの定義\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 5\n",
    "hidden_dim = 3\n",
    "hidden_start = torch.zeros((1, hidden_dim)).T\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "We = torch.randn((hidden_dim, embedding_dim))\n",
    "Wh = torch.randn((hidden_dim, hidden_dim))\n",
    "Wy = torch.randn((vocab_size, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNの順伝播の計算の様子\n",
    "h_t_minus_1 = hidden_start\n",
    "input_history = []\n",
    "\n",
    "# 前のステップの計算が次のステップの計算に影響するため並列化が難しい。\n",
    "# RNNの計算複雑度 len(model_input.tolist()) * hidden_dim * hidden_dim = T * d * d\n",
    "for t in range(len(model_input.tolist())):\n",
    "    idx = model_input.tolist()[t]\n",
    "    input_history.append(idx_to_ch[idx])\n",
    "    print(input_history, '----> RNN ----> ', idx_to_ch[model_target.tolist()[t]])\n",
    "    x = torch.LongTensor([idx])\n",
    "    x = word_embedding_table[x].T\n",
    "    # Attentionを導入したいポイント、過去の文脈がh_t_minus_1に押し込まれる\n",
    "    # ネットワークが単語方向に深くなるため学習が不安定に\n",
    "    # d * d, 系列長によらず一定\n",
    "    h_t = torch.tanh(torch.matmul(We, x) + torch.matmul(Wh, h_t_minus_1))\n",
    "    logits = torch.matmul(Wy, h_t)\n",
    "    print(f'P({idx_to_ch[model_target.tolist()[t]]} | {\", \".join(input_history)}) = {torch.softmax(logits, dim=0).squeeze().tolist()[model_target.tolist()[t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[t]] = 1\n",
    "    h_t_minus_1 = h_t\n",
    "    output_dist = [float('{:.3f}'.format(output)) for output in torch.softmax(logits, dim=0).squeeze().tolist()]\n",
    "    print(f'{output_dist} <-----学習によって近づける-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNによるモデル化によって\n",
    "- 文脈長を固定せずに、任意の長さの文脈を考慮\n",
    "- 文脈長が増えてもパラメータ数が増えない\n",
    "\n",
    "残る課題\n",
    "- 学習が不安定(勾配消失、勾配爆発)\n",
    "- 並列化ができず、学習が遅い\n",
    "- 文脈長が長くなるとトークンの長距離依存関係の把握が難しくなる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformerによるモデル化と学習"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention $(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "Attention Is All You Needの式(1)より\n",
    "\n",
    "$\\operatorname{FFN}(x)=\\max \\left(0, x W_1+b_1\\right) W_2+b_2$  \n",
    "Attention Is All You Needの式(2)より"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "vocab = {'あ': 0, 'い': 1, 'う':2, 'え':3, 'お':4, '<sos>':5, '<eos>':6}\n",
    "idx_to_ch = dict((v, k) for (k,v) in vocab.items())\n",
    "text = 'いえい'\n",
    "text = ['<sos>'] + list(text) + ['<eos>']\n",
    "text = [vocab[ch] for ch in text]\n",
    "model_input = torch.tensor(text[:-1])\n",
    "model_target = torch.tensor(text[1:])\n",
    "print('model input', model_input.tolist(), [idx_to_ch[idx] for idx in model_input.tolist()])\n",
    "print('model target', model_target.tolist(), [idx_to_ch[idx] for idx in model_target.tolist()])\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "d_model = embedding_dim = 3 # Attention is all you needの論文では512\n",
    "d_ff = d_model * 4\n",
    "n_head = 1 # Attention is all you needの論文では8\n",
    "d_k = int(d_model / n_head)\n",
    "d_v = int(d_model / n_head)\n",
    "\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "Wq = torch.randn((d_k, embedding_dim))\n",
    "Wk = torch.randn((d_k, embedding_dim))\n",
    "\n",
    "Wv = torch.randn((d_v, embedding_dim))\n",
    "Wo = torch.randn((d_model, d_v * n_head))\n",
    "\n",
    "Wff_1 = torch.randn((d_ff, d_model))\n",
    "Wff_2 = torch.randn((d_model, d_ff))\n",
    "Wy = torch.randn((vocab_size, d_model))\n",
    "\n",
    "print('self-attentionのパラメータ数', d_model * d_k * n_head * 2 + d_model * d_v * n_head + d_model * d_v * n_head)\n",
    "print('self-attentionのWvのパラメータ数', d_model * d_v * n_head)\n",
    "print('feed-forwardのパラメータ数', d_ff * d_model * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 並列化されていないSelf-Attention層、FeedForward層の計算\n",
    "quries = []\n",
    "keys = []\n",
    "values = []\n",
    "attention_scores = []\n",
    "attention_outputs = []\n",
    "input_history = []\n",
    "\n",
    "# Self-Attentionの計算複雑度 len(model_input.tolist()) * len(model_input.tolist()) * d_model = T * T * d\n",
    "for t in range(len(model_input.tolist())):\n",
    "    idx = model_input.tolist()[t]\n",
    "    input_history.append(idx_to_ch[idx])\n",
    "    x = torch.LongTensor([idx])\n",
    "    x = word_embedding_table[x].T # + positional_encodings\n",
    "    \n",
    "    # single-head, multi-headになると複数の観点でquery, key, valueの発行をする\n",
    "    query_t = torch.matmul(Wq, x) # 〇〇探してます！ 例: token_0: 自分主語です、目的語とか助詞とか動詞とか探してます！\n",
    "\n",
    "    key_t = torch.matmul(Wk, x) # 〇〇持ってます！ 例: token_0: 自分主語です！ token_1: 自分動詞です!\n",
    "    value_t = torch.matmul(Wv, x) # 中身の詳細です！ 例: token_0: 「拙者」です、珍しい1人称です、お侍さんとかが使ったりします token_1: 「食べる」です、食べ物を口に入れる行為です\n",
    "    \n",
    "    # cross attentionの場合はkey, valueは\n",
    "    # key_t = torch.matmul(Wk, another_modality_x)\n",
    "    # value_t = torch.matmul(Wv, another_modality_x)のようになる\n",
    "\n",
    "    quries.append(query_t)\n",
    "    keys.append(key_t)\n",
    "    values.append(value_t)\n",
    "    # Day2の演習では文章単位でベクトル化したqueryとkeyの内積(類似度)をもとにRetrievalを行った\n",
    "    # 計算複雑度 T * d、系列長によって変化\n",
    "    # 遠く離れた過去の文脈を考慮できる、入力データに応じて相互作用し、その結果が後の深い層でも反映される\n",
    "    attention_score = torch.matmul(query_t.T, torch.stack(keys)) / torch.sqrt(torch.tensor(d_k))\n",
    "    attention_score = torch.softmax(attention_score, dim=0)\n",
    "    attention_scores.append(attention_score.squeeze())\n",
    "    self_attention_output = 0\n",
    "    for i in range(len(attention_score)):\n",
    "        # query、keyのコミュニケーションの結果がvalueに反映される(attention scoreで重み付け)\n",
    "        self_attention_output += attention_score[i] * values[i]\n",
    "    attention_outputs.append(self_attention_output)\n",
    "\n",
    "    # FeedForwardの計算\n",
    "    ff_output = torch.matmul(Wff_2, torch.relu(torch.matmul(Wff_1, self_attention_output)))\n",
    "    # 次単語予測、タスク特化のための分類器を作るような場合には新しくWyを用意して学習する\n",
    "    logits = torch.matmul(Wy, ff_output)\n",
    "    output_dist = [float('{:.3f}'.format(output)) for output in torch.softmax(logits, dim=0).squeeze().tolist()]\n",
    "    \n",
    "    print(f'P({idx_to_ch[model_target.tolist()[t]]} | {\", \".join(input_history)}) = {torch.softmax(logits, dim=0).squeeze().tolist()[model_target.tolist()[t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[t]] = 1\n",
    "    print(f'{output_dist} <-----学習によって近づける-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentionの可視化\n",
    "attention_map = torch.zeros((len(model_input.tolist()), len(model_input.tolist())))\n",
    "for t in range(len(model_input.tolist())):\n",
    "    attention_map[t][:t+1] = attention_scores[t]\n",
    "    print('token', input_history)\n",
    "    print(input_history[t], attention_map[t].tolist())\n",
    "    print('-----' * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attentionで必要になるメモリ len(model_input.tolist()) * len(model_input.tolist()) = T * T\n",
    "print(attention_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未来の情報を用いないようにCausal Attentionを用いて並列化(行列の計算にする)\n",
    "input_ids = model_input.tolist()\n",
    "x = torch.LongTensor([input_ids])\n",
    "x = word_embedding_table[x] # + positional_encodings\n",
    "# print(x.shape) # (1, T, embedding_dim)\n",
    "\n",
    "queries = torch.matmul(x.squeeze(), Wq.T) # (T, d_k)\n",
    "keys = torch.matmul(x.squeeze(), Wk.T) # (T, d_k)\n",
    "values = torch.matmul(x.squeeze(), Wv.T) # (T, d_v)\n",
    "# print(queries.shape, keys.shape, values.shape)\n",
    "\n",
    "attention_scores = torch.matmul(queries, keys.T) / torch.sqrt(torch.tensor(d_k)) # (T, T)\n",
    "# causal attention、未来の情報を用いないようにする\n",
    "attention_mask = torch.tril(torch.ones((len(model_input.tolist()), len(model_input.tolist())))) # (T, T)\n",
    "attention_scores = attention_scores.masked_fill(attention_mask==0, float('-inf'))\n",
    "\n",
    "attention_scores = torch.softmax(attention_scores, dim=1) # (T, T)\n",
    "attention_outputs = torch.matmul(attention_scores, values) # (T, d_v)\n",
    "\n",
    "# FeedForwardの計算\n",
    "ff_output = torch.matmul(torch.relu(torch.matmul(attention_outputs, Wff_1.T)), Wff_2.T) # (T, d_model)\n",
    "# print(ff_output.shape)\n",
    "# 次単語予測\n",
    "logits = torch.matmul(ff_output, Wy.T) # (T, vocab_size)\n",
    "output_dists = torch.softmax(logits, dim=1) # (T, vocab_size)\n",
    "\n",
    "for step_t, output_dist in enumerate(output_dists):\n",
    "    print(f'P({idx_to_ch[model_target.tolist()[step_t]]} | {\", \".join(input_history[:step_t + 1])}) = {output_dist[model_target.tolist()[step_t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[step_t]] = 1\n",
    "    print(f'{[float(\"{:.3f}\".format(output)) for output in output_dist.tolist()]} <-----学習によって近づける-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = torch.matmul(queries, keys.T) / torch.sqrt(torch.tensor(d_k)) # (T, T)\n",
    "print('causal attention前')\n",
    "print(attention_scores)\n",
    "# causal attentionの場合はattention_maskを用いて未来の情報をマスクする\n",
    "attention_mask = torch.tril(torch.ones((len(model_input.tolist()), len(model_input.tolist())))) # (T, T)\n",
    "# 下三角行列で未来の情報をマスク\n",
    "print(attention_mask)\n",
    "# 未来の情報はscoreが0になるように-infを代入(softmaxで0になる)\n",
    "attention_scores = attention_scores.masked_fill(attention_mask==0, float('-inf'))\n",
    "print('causal attention後')\n",
    "print(attention_scores)\n",
    "attention_scores = torch.softmax(attention_scores, dim=1) # (T, T)\n",
    "print('softmax後')\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の実装に加え、学習の安定化のためにResidual ConnectionとLayer Normalizationを追加することでTransformerのblockを実装できます。\n",
    "以下にGPT-2の実装と学習のコードを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2の実装 code from https://github.com/karpathy/makemore/tree/master\n",
    "# https://github.com/karpathy/makemore/blob/master/makemore.py\n",
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Andrej Karpathy\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Transformer Language Model (*exactly* as used in GPT-2)\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual connection\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # residual connection\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"トランスフォーマーのパラメーター数: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習2: Transformerのモデルサイズなどのパラメータを変更して未来の単語の予測精度向上を試みる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_datasets, InfiniteDataLoader\n",
    "train_dataset, test_dataset = create_datasets('neco.txt')\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_output_length()\n",
    "\n",
    "############ ハイパーパラメーター\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "n_embd2 = 64\n",
    "lr = 5e-4\n",
    "weight_decay = 0.01\n",
    "batch_size = 2\n",
    "max_steps = 10\n",
    "# ハイパーパラメーター ############\n",
    "\n",
    "config = ModelConfig(\n",
    "    vocab_size=vocab_size, \n",
    "    block_size=block_size,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd, \n",
    "    n_embd2=n_embd2\n",
    ")\n",
    "\n",
    "model = Transformer(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# init optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "# init dataloader\n",
    "batch_loader = InfiniteDataLoader(train_dataset, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "\n",
    "# training loop\n",
    "best_loss = None\n",
    "step = 0\n",
    "stepi = []\n",
    "lossi = []\n",
    "while True:\n",
    "    # get the next batch, ship to device, and unpack it to input and target\n",
    "    batch = batch_loader.next()\n",
    "    batch = [t.to(device) for t in batch]\n",
    "    X, Y = batch\n",
    "\n",
    "    # feed into the model\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    # calculate the gradient, update the weights\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "\n",
    "    stepi.append(step)\n",
    "    lossi.append(loss.log10().item())\n",
    "    step += 1\n",
    "    # termination conditions\n",
    "    if max_steps >= 0 and step >= max_steps:\n",
    "        break\n",
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transoformer言語モデルによる生成\n",
    "from utils import generate\n",
    "context = '吾輩（わ'\n",
    "print('入力: ', context)\n",
    "X_init = train_dataset.encode(context).to(device).unsqueeze(0)\n",
    "top_k = 1\n",
    "# steps = train_dataset.get_output_length() - 1 \n",
    "steps = 20\n",
    "X_samp = generate(model, X_init, steps, top_k=top_k, do_sample=True).to('cpu')\n",
    "row = X_samp[0].tolist()\n",
    "crop_index = row.index(0) if 0 in row else len(row)\n",
    "row = row[:crop_index]\n",
    "print('生成結果: ', train_dataset.decode(row)[:100])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- [Andrej Karpathy(元TeslaのAIチームのリーダー、現在はOpenAI)によるGPT-2実装までの講義動画](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
    "- [CS224N: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "    - https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture08-transformers.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
