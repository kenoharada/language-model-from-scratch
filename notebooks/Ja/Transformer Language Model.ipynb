{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kenoharada/language-model-from-scratch/blob/main/notebooks/Ja/Transformer%20Language%20Model.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training\n",
    "- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "- è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã¯\n",
    "- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãªã„æ‰‹æ³•(uni-gram, bi-gram)\n",
    "- Transformerä»¥å‰ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "    - MLP\n",
    "    - RNN\n",
    "- Transformerã‚’ä½¿ç”¨ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "    - Self-Attentionã¨Feedforward Networkã®å®Ÿè£…ã€ä¸¦åˆ—åŒ–\n",
    "    - GPT-2ã®å®Ÿè£…\n",
    "\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®è§£èª¬ãƒ»ã‚¨ãƒ©ãƒ¼è§£æ¶ˆã‚„ç”¨èªã®è§£èª¬GPT  \n",
    "https://chatgpt.com/g/g-H1Baw636t-mlxian-bei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install japanize-matplotlib -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "ChatGPTã®ã‚ˆã†ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ã¯æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’å¤§é‡ã«é›†ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚  \n",
    "ãƒ‡ãƒ¼ã‚¿ã®é›†ã‚æ–¹ã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒå¤§ããå·¦å³ã•ã‚Œã‚‹ã®ã§ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¯é‡è¦ãªå·¥ç¨‹ã§ã™ã€‚ä»¥ä¸‹ã«å‚è€ƒã«ãªã‚Šãã†ãªè«–æ–‡ã‚’è¼‰ã›ã¦ãŠãã¾ã™ã€‚  \n",
    "- [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159)  \n",
    "- [FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)  \n",
    "- [Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance](https://arxiv.org/abs/2403.16952)  \n",
    "- [To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)  \n",
    "- [Instruction Pre-Training: Language Models are Supervised Multitask Learners](https://arxiv.org/abs/2406.14491)  \n",
    "- [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)  \n",
    "- [Physics of Language Models: Part 3.1, Knowledge Storage and Extraction](https://arxiv.org/abs/2309.14316)  \n",
    "- [æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã§é–‹å‚¬ã—ãŸLLMå‹‰å¼·ä¼šã§ã®ç™ºè¡¨è³‡æ–™](https://docs.google.com/presentation/d/14SeP11PcgmNcl93Xt0ziSynxdrOVd2WM/edit?usp=sharing&ouid=101802221278095300433&rtpof=true&sd=true)  \n",
    "\n",
    "ä»Šå›ã¯[LLM-jpã¨ã„ã†å›£ä½“](https://llm-jp.nii.ac.jp/)ãŒæ•´å‚™ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚\n",
    "- https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/train_9.jsonl.gz\n",
    "# !wget --no-check-certificate https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2/-/raw/main/ja/ja_wiki/validation_0.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('train_9.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # å„è¡Œã‚’JSONã¨ã—ã¦èª­ã¿è¾¼ã‚€\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_text += data['text'] + \"\\n\"\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "print(data.keys())\n",
    "print(data['text'][:100])\n",
    "print(data['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('validation_0.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # å„è¡Œã‚’JSONã¨ã—ã¦èª­ã¿è¾¼ã‚€\n",
    "        data = json.loads(line)\n",
    "        val_data_text += data['text'] + \"\\n\"\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break\n",
    "print(data.keys())\n",
    "print(data['text'][:100])\n",
    "print(data['meta'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã¯ã€€ã€€\n",
    "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®\"è¨€èªãƒ¢ãƒ‡ãƒ«\"ã¨ã¯ã€å˜èªåˆ—ã®å‡ºç¾ç¢ºç‡ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚  \n",
    "ç¢ºç‡ã‚’è¨ˆç®—ã§ãã‚‹ã®ã§ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ãŒã‚ˆãè¦‹ã‚‹æ–‡ç« (ç¢ºç‡ãŒé«˜ã„)ãªã®ã‹ã€å¤‰ãªæ–‡ç« ãªã®ã‹(ç¢ºç‡ãŒä½ã„)ã‚’åˆ¤æ–­ã™ã‚‹ã“ã¨ãŒã§ããŸã‚Šã€æ–°ãŸã«æ–‡ç« ã‚’ç”Ÿæˆ(ç¢ºç‡ã«å¾“ã£ã¦ãã˜å¼•ãã‚’ã™ã‚‹)ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "ã‚ˆã‚Šè‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã®ãŸã‚ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã¨ã©ã®ã‚ˆã†ã«ç¢ºç‡ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã‹ã®ãƒ‡ã‚¶ã‚¤ãƒ³ãŒé‡è¦ã§ã™ã€‚\n",
    "### ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦\n",
    "è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«å­¦ç¿’ã™ã‚‹ãŸã‚ã€ãã®ãƒ‡ãƒ¼ã‚¿ã«æœ‰ç›Šãªæƒ…å ±(ä¾‹ãˆã°æ—¥æœ¬ã®æ­´å²ã‚„æ³•å¾‹ã«é–¢ã™ã‚‹æ–‡ç« )ãŒå«ã¾ã‚Œã¦ã„ãªã„ã¨ã€å˜èªè‡ªèº«ã®ç†è§£ã‚„å˜èªåŒå£«ã®é–¢ä¿‚æ€§ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚  \n",
    "äººé–“ãŒæœ¬ã‚’èª­ã‚“ã ã‚Šã€äººã¨ã®ä¼šè©±ã‚’é€šã˜ã¦æ–°ã—ã„çŸ¥è­˜ã‚’å¾—ãŸã‚Šã€è‰¯ã„æ–‡ç« ã®æ›¸ãæ–¹ã‚’å­¦ã‚“ã ã‚Šã™ã‚‹ã‚ˆã†ã«ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚‚ãƒ‡ãƒ¼ã‚¿ã‚’é€šã˜ã¦å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "### ãƒ¢ãƒ‡ãƒ«åŒ–ã«ã¤ã„ã¦\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚’å¾—ãŸã¨ã—ã¦ã‚‚ã€ã©ã®ã‚ˆã†ã«ç¢ºç‡ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã‹ã®ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã†ã¾ãã„ã‹ãªã„ã¨ã€è‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ä½œã‚Œã¾ã›ã‚“ã€‚\n",
    "- ãƒ‡ãƒ¼ã‚¿ä¸­ã®æ–‡å­—ã‚’æ•°ãˆä¸Šã’ã¦å‰ã®1å˜èªã‹ã‚‰æ¬¡ã®1å˜èªã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸€ç¨®ã§ã‚ã‚‹MLPã‚’ç”¨ã„ã¦ã€å‰ã®æ•°å˜èªã‹ã‚‰æ¬¡ã®1å˜èªã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "- Transformerã‚’ç”¨ã„ã¦ã€ã‚ˆã‚Šé•·ã„æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«  \n",
    "\n",
    "ã‚’é–‹ç™ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãªã„ã€æ•°ãˆä¸Šã’ã«ã‚ˆã‚‹æ‰‹æ³•(uni-gram, bi-gram)ã®è¨€èªãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram è¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "$P\\left(w_1, \\ldots, w_m\\right)=\\prod_{i=1}^{i=m} P\\left(w_i \\mid w_1, \\ldots, w_{i-1}\\right) \\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-n}, \\ldots, w_{i-1}\\right)$\n",
    "\n",
    "#### uni-gramãƒ¢ãƒ‡ãƒ«\n",
    "$P\\left(w_1, \\ldots, w_m\\right) \\approx \\prod_{i=1}^{i=m} P(w_i)$\n",
    "\n",
    "#### bi-gramãƒ¢ãƒ‡ãƒ«  \n",
    "$P\\left(w_1, \\ldots, w_m\\right)\\approx \\prod_{i=1}^{i=m} P\\left(w_i \\mid w_{i-1}\\right)$\n",
    "\n",
    "$P\\left(w_{i} \\mid w_{i-1}\\right)=\\frac{\\operatorname{count}\\left(w_{i}, w_{i-1}\\right)}{\\operatorname{count}\\left(w_{i}\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mini_train_data_text[:100])\n",
    "print('------------------------')\n",
    "print('å…¨ä½“ã®æ–‡å­—æ•°: ', len(mini_train_data_text))\n",
    "print('æ–‡å­—ã®ç¨®é¡: ', len(set(mini_train_data_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_count = {}\n",
    "for character in mini_train_data_text:\n",
    "    character_count[character] = character_count.get(character, 0) + 1\n",
    "# å‡ºç¾é »åº¦ã®é«˜ã„æ–‡å­—ã‚’ä¸Šä½5å€‹è¡¨ç¤º\n",
    "print('å‡ºç¾é »åº¦ã®é«˜ã„æ–‡å­—')\n",
    "print(sorted(character_count.items(), key = lambda kv: -kv[1])[:5])\n",
    "\n",
    "# éå»ã®æƒ…å ±ã‚’è€ƒæ…®ã—ãªã„uni-gramãƒ¢ãƒ‡ãƒ«\n",
    "print()\n",
    "total_count = sum(character_count.values())\n",
    "ch_0 = 'æ—¥'\n",
    "ch_1 = 'æœ¬'\n",
    "ch_2 = 'æ›œ'\n",
    "print(f\"'{ch_0}'ã®ç¢ºç‡ = '{ch_0}'ã®å‡ºç¾å›æ•° Ã· å…¨ä½“ã®æ–‡å­—æ•° =  {character_count[ch_0]} Ã· {total_count} = {character_count[ch_0] / total_count}\")\n",
    "print(f\"'{ch_0 + ch_1}'ã®ç¢ºç‡ = '{ch_0}'ã®ç¢ºç‡ x '{ch_1}'ã®ç¢ºç‡ = {character_count[ch_0] / total_count} x {character_count[ch_1] / total_count} = {character_count[ch_0] * character_count[ch_1] / total_count ** 2}\")\n",
    "print(f\"'{ch_0 + ch_2}'ã®ç¢ºç‡ = '{ch_0}'ã®ç¢ºç‡ x '{ch_2}'ã®ç¢ºç‡ = {character_count[ch_0] / total_count} x {character_count[ch_2] / total_count} = {character_count[ch_0] * character_count[ch_2] / total_count ** 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡ºç¾ç¢ºç‡ã®å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "total_count = sum(character_count.values())\n",
    "# ä¸Šä½30æ–‡å­—ã®å‡ºç¾é »åº¦ã‚’å–ã‚‹\n",
    "top_k = 30\n",
    "top_k_key = sorted(character_count.keys(), key = lambda k: -character_count[k])[:top_k]\n",
    "top_k_value = [character_count[k] for k in top_k_key]\n",
    "top_k_probability = [v / total_count for v in top_k_value]\n",
    "\n",
    "# xè»¸ã«æ–‡å­—ã€yè»¸ã«å‡ºç¾é »åº¦ã‚’å–ã‚‹\n",
    "plt.bar(top_k_key, top_k_probability)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ\n",
    "# greedy decoding\n",
    "max_tokens = 20\n",
    "generated_text = 'ã€å‹ã¤ã‹'\n",
    "for _ in range(max_tokens):\n",
    "    next_char = max(character_count, key=lambda k: character_count[k]) # æœ€ã‚‚å‡ºç¾é »åº¦ãŒé«˜ã„æ–‡å­—ã‚’é¸æŠ, greedy decoding\n",
    "    print(repr(next_char))\n",
    "    generated_text += next_char\n",
    "\n",
    "print(repr(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_p sampling\n",
    "import random\n",
    "random.seed(1234)\n",
    "max_tokens = 20\n",
    "top_p = 0.9\n",
    "generated_text = 'ã€å‹ã¤ã‹'\n",
    "for _ in range(max_tokens):\n",
    "    sorted_character_count = sorted(character_count.items(), key=lambda kv: -kv[1])\n",
    "    cumulative_probability = 0\n",
    "    vocab = []\n",
    "    vocab_count = []\n",
    "    vocab_total_count = 0\n",
    "    for character, count in sorted_character_count:\n",
    "        cumulative_probability += count / total_count\n",
    "        if cumulative_probability > top_p:\n",
    "            break\n",
    "        vocab.append(character)\n",
    "        vocab_count.append(count)\n",
    "        vocab_total_count += count\n",
    "    probablity = [count / vocab_total_count for count in vocab_count]\n",
    "    next_char = random.choices(vocab, probablity)[0]\n",
    "    print(repr(next_char))\n",
    "    generated_text += next_char\n",
    "print(repr(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Tokenizerã®è¨­è¨ˆã‚‚é‡è¦ã§ã™ã€ä»¥ä¸‹ã«å‚è€ƒã¨ãªã‚‹ãƒªãƒ³ã‚¯ã‚’è¼‰ã›ã¦ãŠãã¾ã™ã€‚  \n",
    "- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)  \n",
    "- [Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)  \n",
    "- [ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models](https://arxiv.org/abs/2105.13626)  \n",
    "- GPTã‚·ãƒªãƒ¼ã‚ºã®Tokenizer: https://github.com/openai/tiktoken\n",
    "- Llama2ã‚„LLM-jpã®tokenizerã‚’ä½œæˆã™ã‚‹éš›ã«åˆ©ç”¨: https://github.com/google/sentencepiece\n",
    "- Byte Pair Encodingã®å®Ÿè£…: https://github.com/kenoharada/language-model-from-scratch/blob/main/notebooks/Ja/Tokenizer.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars_in_train_text = sorted(list(set(mini_train_data_text)))\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, chars):\n",
    "        self.str_to_idx = dict()\n",
    "        self.str_to_idx['<|endoftext|>'] = 0\n",
    "        # utf-8\n",
    "        for i in range(256):\n",
    "            if f'<utf8_{i}>' not in self.str_to_idx:\n",
    "                self.str_to_idx[f'<utf8_{i}>'] = len(self.str_to_idx)\n",
    "        for char in chars:\n",
    "            self.str_to_idx[char] = len(self.str_to_idx)\n",
    "        self.idx_to_str = dict()\n",
    "        for key, value in self.str_to_idx.items():\n",
    "            self.idx_to_str[value] = key\n",
    "    \n",
    "    def encode(self, text, eot=False):\n",
    "        result = []\n",
    "        for char in text:\n",
    "            if char not in self.str_to_idx:\n",
    "                utf_8_num = list(char.encode(\"utf-8\"))\n",
    "                for num in utf_8_num:\n",
    "                    result.append(self.str_to_idx[f'<utf8_{num}>'])\n",
    "            else:\n",
    "                result.append(self.str_to_idx[char])\n",
    "        if eot:\n",
    "            result.append(self.str_to_idx['<|endoftext|>'])\n",
    "        return result\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        decoded_with_utf_token = [self.idx_to_str[token] for token in tokens]\n",
    "        decoded_postprocess_utf = []\n",
    "        utf_tokens = []\n",
    "        for token in decoded_with_utf_token:\n",
    "            if token.startswith(\"<utf8_\"):\n",
    "                utf_num = int(token.replace(\"<utf8_\", \"\").replace(\">\", \"\"))\n",
    "                utf_tokens.append(utf_num)\n",
    "            else:\n",
    "                if utf_tokens:\n",
    "                    decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "                    utf_tokens = []\n",
    "                decoded_postprocess_utf.append(token)\n",
    "        if utf_tokens:\n",
    "            decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "            utf_tokens = []\n",
    "        return \"\".join(decoded_postprocess_utf)\n",
    "    \n",
    "    def decode_with_utf(self, tokens):\n",
    "        return \"\".join([self.idx_to_str[token] for token in tokens])\n",
    "\n",
    "tokenizer = Tokenizer(unique_chars_in_train_text) # Tokenizerã®åˆæœŸåŒ–ã€ä¸€èˆ¬çš„ã«ã¯Byte Pair Encodingã‚„Unigram Language Modelãªã©ã‚’æ´»ç”¨ã—ã¦Tokenizerã‚’å®Ÿè£…ã™ã‚‹\n",
    "text = 'è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‹‰å¼·ã¯æ¥½ã—ã„ã§ã™ã€‚'\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚‹èª\n",
    "'æ—¥' in unique_chars_in_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('æ—¥'), tokenizer.decode_with_utf([1954]), tokenizer.decode([1954])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã«ãªã„æœªçŸ¥èª\n",
    "'ğŸ˜„' in unique_chars_in_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('ğŸ˜„'), tokenizer.decode_with_utf([241, 160, 153, 133]), tokenizer.decode([241, 160, 153, 133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-gramãƒ¢ãƒ‡ãƒ«\n",
    "bigram_count = {}\n",
    "mini_train_data_tokens = []\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "\n",
    "with gzip.open('train_9.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # å„è¡Œã‚’JSONã¨ã—ã¦èª­ã¿è¾¼ã‚€\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "\n",
    "val_data_tokens = []\n",
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('validation_0.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # å„è¡Œã‚’JSONã¨ã—ã¦èª­ã¿è¾¼ã‚€\n",
    "        data = json.loads(line)\n",
    "        val_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break\n",
    "\n",
    "for i in range(len(mini_train_data_tokens) - 1):\n",
    "    bigram = (mini_train_data_tokens[i], mini_train_data_tokens[i+1])\n",
    "    bigram_count[bigram] = bigram_count.get(bigram, 0) + 1\n",
    "\n",
    "# top-kã®bigramã‚’è¡¨ç¤º\n",
    "print('å‡ºç¾é »åº¦ã®é«˜ã„bigram')\n",
    "top_k = 10\n",
    "top_k_bigram = sorted(bigram_count.items(), key = lambda kv: -kv[1])[:top_k]\n",
    "print(top_k_bigram)\n",
    "# decodeã—ã¦ç¢ºèª\n",
    "for bigram, count in top_k_bigram:\n",
    "    print(repr(tokenizer.decode([bigram[0]])), repr(tokenizer.decode([bigram[1]])), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ\n",
    "# greedy decoding\n",
    "max_tokens = 20\n",
    "generated_text = 'ã€å‹ã¤ã‹'\n",
    "generated_tokens = tokenizer.encode(generated_text)\n",
    "for _ in range(max_tokens):\n",
    "    next_token = max(bigram_count, key=lambda k: bigram_count[k] if k[0] == generated_tokens[-1] else 0) # ç›´å‰ã®æ–‡å­—ãŒæ¥ãŸã¨ãã®æ¬¡ã®æ–‡å­—ã®å‡ºç¾é »åº¦ãŒæœ€ã‚‚é«˜ã„ã‚‚ã®ã‚’é¸æŠ\n",
    "    generated_tokens.append(next_token[1])\n",
    "    print(repr(tokenizer.decode([next_token[1]])))\n",
    "print(generated_tokens)\n",
    "print(tokenizer.decode(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "class Ngram:\n",
    "    def __init__(self, n, vocab, laplace=1):\n",
    "        self.n = n\n",
    "        self.vocab = vocab\n",
    "        self.laplace = laplace\n",
    "        self.ngram = defaultdict(lambda: laplace)\n",
    "        self.context_count = defaultdict(lambda: laplace * len(self.vocab))\n",
    "    \n",
    "    def train(self, token_list):\n",
    "        assert isinstance(token_list, list)\n",
    "        for i in range(len(token_list) - self.n + 1):\n",
    "            ngram_list = copy.deepcopy(token_list[i:i+self.n])\n",
    "            ngram_list = [str(i) for i in ngram_list]\n",
    "            context = ngram_list[:-1]\n",
    "            ngram_key = '-'.join(ngram_list)\n",
    "            context_key = '-'.join(context)\n",
    "            self.ngram[ngram_key] += 1\n",
    "            self.context_count[context_key] += 1\n",
    "    \n",
    "\n",
    "    def train_batch(self, token_list):\n",
    "        for tokens in token_list:\n",
    "            self.train(tokens)\n",
    "    \n",
    "    def get_prob(self, ngram):\n",
    "        if self.n == 1:\n",
    "            return self.ngram[ngram] / len(self.vocab)\n",
    "        else:\n",
    "            context = ngram.split('-')[:-1]\n",
    "            context = '-'.join(context)\n",
    "            return self.ngram[ngram] / self.context_count[context]\n",
    "    \n",
    "    def get_prob_distribution(self, n_minus_1_gram):\n",
    "        distribution = []\n",
    "        distribution_dict = {}\n",
    "        for word in self.vocab:\n",
    "            ngram_list = n_minus_1_gram + [word]\n",
    "            ngram = '-'.join([str(i) for i in ngram_list])\n",
    "            # print('hi', ngram)\n",
    "            distribution.append(self.get_prob(ngram))\n",
    "            distribution_dict[word] = self.get_prob(ngram)\n",
    "        return distribution, distribution_dict\n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        if isinstance(token_indexes, torch.Tensor) or isinstance(token_indexes, torch.LongTensor):\n",
    "            token_indexes = token_indexes.tolist()\n",
    "        assert isinstance(token_indexes, list)\n",
    "        batch_size = len(token_indexes)\n",
    "        sequence_length = len(token_indexes[0])\n",
    "        distributions = torch.ones(batch_size, sequence_length, len(self.vocab))\n",
    "        distributions /= len(self.vocab)\n",
    "        for i in range(sequence_length):\n",
    "            for batch in range(batch_size):\n",
    "                if self.n == 2:\n",
    "                    context = [token_indexes[batch][i]]\n",
    "                else:\n",
    "                    if i < self.n - 1:\n",
    "                        if i == 0:\n",
    "                            context = [token_indexes[batch][i]]\n",
    "                        else:\n",
    "                            context = token_indexes[batch][:i+1]\n",
    "                    else:\n",
    "                        context = token_indexes[batch][i-self.n+2:i+1]\n",
    "                distribution, _ = self.get_prob_distribution(context)\n",
    "                distributions[batch, i] = torch.tensor(distribution)\n",
    "        # distributions: (batch_size, sequence_length, vocab_size)\n",
    "        return distributions\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        distributions = self.forward(token_indexes)\n",
    "        distributions = distributions.to(targets.device)\n",
    "        log_distributions = torch.log(distributions)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = log_distributions.shape\n",
    "        loss = F.nll_loss(\n",
    "            log_distributions.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_length=10):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        for _ in range(max_length):\n",
    "            distributions = self.forward(token_indexes)\n",
    "            distributions = distributions[0, -1]\n",
    "            # greedy decoding\n",
    "            next_token = torch.argmax(distributions).item()\n",
    "            token_indexes[0].append(next_token)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "parameters = len(tokenizer.str_to_idx) ** n\n",
    "print(f'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = Ngram(2, tokenizer.str_to_idx.values())\n",
    "ngram.train(mini_train_data_tokens)\n",
    "generated_text = 'ã€å‹ã¤ã‹'\n",
    "context_token_indexes = [tokenizer.encode(generated_text)]\n",
    "generated_tokens = ngram.generate(context_token_indexes, max_length=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1:context_length+1]\n",
    "ngram.loss([input_tokens], torch.tensor([target_tokens])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = Ngram(4, tokenizer.str_to_idx.values())\n",
    "ngram.train(mini_train_data_tokens)\n",
    "generated_text = 'ã€å‹ã¤ã‹'\n",
    "context_token_indexes = [tokenizer.encode(generated_text)]\n",
    "generated_tokens = ngram.generate(context_token_indexes, max_length=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1:context_length+1]\n",
    "ngram.loss([input_tokens], torch.tensor([target_tokens])).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã€€ã€€\n",
    "è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ã‚¶ã‚¤ãƒ³ã™ã‚‹éš›ã«ã€éå»ã®æ–‡è„ˆã‚’è€ƒæ…®ã™ã‚‹ã¨ã‚ˆã‚Šè‡ªç„¶ãªæ–‡ç« ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒã§ãã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚  \n",
    "ãŸã ã€N-gramãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã¯ã€$O\\left(\\left|V\\right|^n\\right)$ã€€ã¨ãªã‚Šã€éå»ã®æ–‡è„ˆãŒå¢—ãˆã‚Œã°å¢—ãˆã‚‹ã»ã©çµ„ã¿åˆã‚ã›ãŒè†¨å¤§ã«ãªã‚Šã¾ã™ã€‚  \n",
    "çµ„ã¿åˆã‚ã›å˜ä½ã§æ•°ãˆä¸Šã’ã¦ã„ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ä¸­ã«å‡ºç¾ã—ãªã„çµ„ã¿åˆã‚ã›ãŒã‚ã‚‹ã¨ã€ãã®çµ„ã¿åˆã‚ã›ã®ç¢ºç‡ã¯0ã¨ãªã‚Šã€ãã®å¾Œã®äºˆæ¸¬ãŒã§ããªããªã£ã¦ã—ã¾ã„ã¾ã™ã€‚  \n",
    "\n",
    "ã¾ãŸã€çµ„ã¿åˆã‚ã›åˆ¥ã®æ•°ãˆä¸Šã’ã§ã¯ã€å˜èªãƒ»æ–‡è„ˆã‚’ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸã‚‚ã®ã¨ã—ã¦æ‰±ã£ã¦ãŠã‚Šã€å˜èªã®æ„å‘³ã‚„æ–‡è„ˆã‚’å…±æœ‰ã—ãŸè¡¨ç¾ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã€‚\n",
    "\n",
    "è§£æ±ºç­–ã¨ã—ã¦çµ„ã¿åˆã‚ã›ã®è¡¨ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«åŒ–ã§ã¯ãªãã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®è¡¨ç¾ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«åŒ–ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ã®éš›ã«è¡Œã†ã€æ¬¡å˜èªäºˆæ¸¬ã«ã‚ˆã£ã¦å˜èªãƒ™ã‚¯ãƒˆãƒ«ã«ã¯å˜èªã®æ„å‘³ã‚„æ¦‚å¿µãŒä»˜ä¸ã•ã‚Œã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯å˜èªåŒå£«ã®é–¢ä¿‚æ€§ãŒå­¦ç¿’ã•ã‚Œã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "æ—¥æœ¬ã§æœ€ã‚‚é«˜ã„å±±ã¯ã€Œï¼Ÿã€ã®ï¼Ÿã‚’å½“ã¦ã‚‹ãŸã‚ã«ã€\n",
    "- ï¼Ÿã«ã¯æ–‡æ³•çš„ã«åè©ãŒå…¥ã‚‹\n",
    "- ãã®å€™è£œã¯æ–‡è„ˆçš„ã«å±±ã§ã‚ã‚Š\n",
    "- æ—¥æœ¬ã§æœ€ã‚‚é«˜ã„ã¨ã„ã†æƒ…å ±ãŒã‚ã‚‹ã€ã¨ã„ã†ã‚ˆã†ãªæ–‡è„ˆä¸Šã®ã©ã®æƒ…å ±ã«ç€ç›®ã™ã‚‹ã‹\n",
    "- æ–‡è„ˆã‚’è¸ã¾ãˆãŸä¸Šã§ä»Šã¾ã§å¾—ãŸçŸ¥è­˜ã‚’ã©ã®ã‚ˆã†ã«çµ„ã¿åˆã‚ã›ã‚‹ã‹\n",
    "\n",
    "ã¨ã„ã†ã“ã¨ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå­¦ç¿’ã—ã¾ã™ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        # FeedForward Layer\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "            )\n",
    "        # vocab_sizeã¸ã®å¤‰æ›\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        print('number of parameters:', sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_index: (batch_size, sequence_length)\n",
    "        embedding = self.token_embedding_table(token_indexes)\n",
    "        logits = self.linear(self.ff(embedding))\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length),\n",
    "            reduction='none'\n",
    "            )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            # greedy decoding\n",
    "            next_token = torch.argmax(next_token_probs, dim=-1, keepdim=True)\n",
    "            # next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "d_model = 4\n",
    "d_ff = d_model * 4\n",
    "nn_lm = BigramLanguageModel(vocab_size, d_model, d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'ã€å‹ã¤ã‹'\n",
    "token_indexes = torch.tensor([tokenizer.encode(text)])\n",
    "input_token_indexes = token_indexes[:, :-1]\n",
    "target_token_indexes = token_indexes[:, 1:]\n",
    "print(input_token_indexes)\n",
    "print(tokenizer.decode(input_token_indexes[0].tolist()))\n",
    "\n",
    "print(target_token_indexes)\n",
    "print(tokenizer.decode(target_token_indexes[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = nn_lm.token_embedding_table(input_token_indexes)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_lm.generate(token_indexes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"å…¥åŠ›: {input_text} â†’ Token id: {input_token_id} â†’ Token embedding: {token_embeddings} \n",
    "â†’ Neural Net â†’ \n",
    "{next_token_prediction_prob} <---å­¦ç¿’ã«ã‚ˆã£ã¦è¿‘ã¥ã‘ã‚‹---> Target: {target_prob}\"\"\"\n",
    "for idx, token in enumerate(input_token_indexes[0]):\n",
    "    input_text = tokenizer.decode([input_token_indexes[0].tolist()[idx]])\n",
    "    input_token_id = token.item()\n",
    "    token_embeddings = nn_lm.token_embedding_table(token.unsqueeze(0)).squeeze().detach().numpy().tolist()\n",
    "    next_token_prediction = nn_lm(token.unsqueeze(0))\n",
    "    next_token_prediction = F.softmax(next_token_prediction, dim=-1)\n",
    "    next_token_prediction_top_k_index = torch.topk(next_token_prediction, 3).indices\n",
    "\n",
    "    target_token_id = target_token_indexes[0][idx].item()\n",
    "    pickup_token_indexes = [target_token_id] + next_token_prediction_top_k_index[0].tolist()\n",
    "    next_token_prediction_prob = next_token_prediction[0][pickup_token_indexes].tolist()\n",
    "    next_token_prediction_prob_text = [f'{token_id}: {prob:.3f}' for token_id, prob in zip(pickup_token_indexes, next_token_prediction_prob)]\n",
    "    next_token_prediction_prob = ', '.join(next_token_prediction_prob_text)\n",
    "    target_token_distribution = F.one_hot(target_token_indexes[0][idx], num_classes=vocab_size)[pickup_token_indexes].tolist()\n",
    "    target_token_distribution = [f'{token_id}: {prob}' for token_id, prob in zip(pickup_token_indexes, target_token_distribution)]\n",
    "    target_token_distribution = ', '.join(target_token_distribution)\n",
    "    print(template.format(input_text=input_text, input_token_id=input_token_id, token_embeddings=token_embeddings, next_token_prediction_prob=next_token_prediction_prob, target_prob=target_token_distribution))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nn_lm = BigramLanguageModel(vocab_size, d_model, d_ff).to(device)\n",
    "optimizer = torch.optim.AdamW(nn_lm.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 1\n",
    "training_tokens = 0\n",
    "print('before training')\n",
    "val_loss = 0\n",
    "for i in range(0, len(val_data_tokens), batch_size):\n",
    "    batch_tokens = val_data_tokens[i:i+batch_size]\n",
    "    input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "    target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "    val_loss += loss.sum().item()\n",
    "val_loss = val_loss / len(val_data_tokens)\n",
    "print(f'val_loss: {val_loss}')\n",
    "print('start training')\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(mini_train_data_tokens), batch_size):\n",
    "        batch_tokens = mini_train_data_tokens[i:i+batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        loss = nn_lm.loss(input_token_indexes, target_token_indexes)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_tokens += len(batch_tokens)\n",
    "        if training_tokens % 10000 == 0:\n",
    "            print(f'epoch: {epoch}, loss: {loss.item()}, training_tokens: {training_tokens}')\n",
    "    val_loss = 0\n",
    "    for i in range(0, len(val_data_tokens), batch_size):\n",
    "        batch_tokens = val_data_tokens[i:i+batch_size]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = nn_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "        val_loss += loss.sum().item()\n",
    "    val_loss = val_loss / len(val_data_tokens)\n",
    "    print(f'epoch: {epoch}, val_loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'ã€å‹ã¤ã‹'\n",
    "context_token_indexes = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "generated_tokens = nn_lm.generate(context_token_indexes, max_new_tokens=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens = val_data_tokens[:context_length]\n",
    "target_tokens = val_data_tokens[1:context_length+1]\n",
    "input_token_indexes = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "target_token_indexes = torch.tensor(target_tokens).unsqueeze(0).to(device)\n",
    "nn_lm.loss(input_token_indexes, target_token_indexes).item()\n",
    "\n",
    "# bi-gram(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° 16516096)ã®æ€§èƒ½: 5.082467079162598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# å½“ã¦ãšã£ã½ã†ãƒ¢ãƒ‡ãƒ«ã®loss\n",
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "print('vocab_size', vocab_size)\n",
    "print(-math.log(1/len(tokenizer.str_to_idx.values())))\n",
    "\n",
    "math.exp(6.014070987701416), math.exp(2.0) # Understanding Emergent Abilities of Language Models from the Loss Perspective, https://arxiv.org/abs/2403.15796"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¼”ç¿’1: MLPã®å±¤ã‚’å¢—ã‚„ã—ãŸã‚Šã€å­¦ç¿’ç‡ãªã©ã‚’å¤‰æ›´ã—ã¦æœªæ¥ã®å˜èªã®äºˆæ¸¬ç²¾åº¦å‘ä¸Šã‚’è©¦ã¿ã‚‹ã€éå»ã®Nãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã¿ã‚ˆã†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE ME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨€èªãƒ™ã‚¯ãƒˆãƒ«ã¨MLPã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«åŒ–ã«ã‚ˆã£ã¦\n",
    "- å¯†ãªè¨€èªãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã«ã‚ˆã£ã¦ã€å˜èªã®æ„å‘³ã‚„æ¦‚å¿µã‚’è¡¨ç¾\n",
    "- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒO(exp(n))ã‹ã‚‰O(n)ã¸ã¨å‰Šæ¸›\n",
    "\n",
    "æ®‹ã‚‹èª²é¡Œ\n",
    "- è¦‹ã‚Œã‚‹éå»ã®æ–‡è„ˆé•·ãŒå›ºå®šã€å¢—ã‚„ãã†ã¨ã™ã‚‹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¢—åŠ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = {'ã‚': 0, 'ã„': 1, 'ã†':2, 'ãˆ':3, 'ãŠ':4, '<|endoftext|>':5}\n",
    "idx_to_ch = dict((v, k) for (k,v) in vocab.items())\n",
    "text = 'ã„ãˆãˆã„'\n",
    "text = list(text) + ['<|endoftext|>']\n",
    "text = [vocab[ch] for ch in text]\n",
    "model_input = torch.tensor(text[:-1])\n",
    "model_target = torch.tensor(text[1:])\n",
    "print('model input', model_input.tolist(), [idx_to_ch[idx] for idx in model_input.tolist()])\n",
    "print('model target', model_target.tolist(), [idx_to_ch[idx] for idx in model_target.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNã®å®šç¾©\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 5\n",
    "hidden_dim = 3\n",
    "hidden_start = torch.zeros((1, hidden_dim)).T\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "We = torch.randn((hidden_dim, embedding_dim))\n",
    "Wh = torch.randn((hidden_dim, hidden_dim))\n",
    "Wy = torch.randn((vocab_size, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNã®é †ä¼æ’­ã®è¨ˆç®—ã®æ§˜å­\n",
    "h_t_minus_1 = hidden_start\n",
    "input_history = []\n",
    "\n",
    "# å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—ãŒæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—ã«å½±éŸ¿ã™ã‚‹ãŸã‚ä¸¦åˆ—åŒ–ãŒé›£ã—ã„ã€‚\n",
    "# RNNã®è¨ˆç®—è¤‡é›‘åº¦ len(model_input.tolist()) * hidden_dim * hidden_dim = T * d * d\n",
    "for t in range(len(model_input.tolist())):\n",
    "    idx = model_input.tolist()[t]\n",
    "    input_history.append(idx_to_ch[idx])\n",
    "    print(input_history, '----> RNN ----> ', idx_to_ch[model_target.tolist()[t]])\n",
    "    x = torch.LongTensor([idx])\n",
    "    x = word_embedding_table[x].T\n",
    "    # Attentionã‚’å°å…¥ã—ãŸã„ãƒã‚¤ãƒ³ãƒˆã€éå»ã®æ–‡è„ˆãŒh_t_minus_1ã«æŠ¼ã—è¾¼ã¾ã‚Œã‚‹\n",
    "    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå˜èªæ–¹å‘ã«æ·±ããªã‚‹ãŸã‚å­¦ç¿’ãŒä¸å®‰å®šã«\n",
    "    # d * d, ç³»åˆ—é•·ã«ã‚ˆã‚‰ãšä¸€å®š\n",
    "    h_t = torch.tanh(torch.matmul(We, x) + torch.matmul(Wh, h_t_minus_1))\n",
    "    logits = torch.matmul(Wy, h_t)\n",
    "    print(f'P({idx_to_ch[model_target.tolist()[t]]} | {\", \".join(input_history)}) = {torch.softmax(logits, dim=0).squeeze().tolist()[model_target.tolist()[t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[t]] = 1\n",
    "    h_t_minus_1 = h_t\n",
    "    output_dist = [float('{:.3f}'.format(output)) for output in torch.softmax(logits, dim=0).squeeze().tolist()]\n",
    "    print(f'{output_dist} <-----å­¦ç¿’ã«ã‚ˆã£ã¦è¿‘ã¥ã‘ã‚‹-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«åŒ–ã«ã‚ˆã£ã¦\n",
    "- æ–‡è„ˆé•·ã‚’å›ºå®šã›ãšã«ã€ä»»æ„ã®é•·ã•ã®æ–‡è„ˆã‚’è€ƒæ…®\n",
    "- æ–‡è„ˆé•·ãŒå¢—ãˆã¦ã‚‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¢—ãˆãªã„\n",
    "\n",
    "æ®‹ã‚‹èª²é¡Œ\n",
    "- å­¦ç¿’ãŒä¸å®‰å®š(å‹¾é…æ¶ˆå¤±ã€å‹¾é…çˆ†ç™º)\n",
    "- ä¸¦åˆ—åŒ–ãŒã§ããšã€å­¦ç¿’ãŒé…ã„\n",
    "- æ–‡è„ˆé•·ãŒé•·ããªã‚‹ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®æŠŠæ¡ãŒé›£ã—ããªã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¼”ç¿’2: RNNã§å­¦ç¿’ã€ç”Ÿæˆã—ã¦ã¿ã‚ˆã†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE ME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformerã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«åŒ–ã¨å­¦ç¿’"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention $(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "Attention Is All You Needã®å¼(1)ã‚ˆã‚Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 4\n",
    "d_head = 2\n",
    "\n",
    "K = torch.randn(batch_size, sequence_length, d_head)\n",
    "Q = torch.randn(batch_size, sequence_length, d_head)\n",
    "V = torch.randn(batch_size, sequence_length, d_head)\n",
    "\n",
    "qk_dot_product = Q @ K.transpose(-2, -1)\n",
    "scaled_qk_dot_product = qk_dot_product / (d_head ** 0.5)\n",
    "\n",
    "attention_scores = torch.softmax(scaled_qk_dot_product, dim=-1)\n",
    "attention_output = attention_scores @ V\n",
    "\n",
    "attention_scores_without_scale = torch.softmax(qk_dot_product, dim=-1)\n",
    "attention_output_without_scale = attention_scores_without_scale @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk_dot_product.var(), scaled_qk_dot_product.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores[:, -1, :], attention_scores[:, -1, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores[:, -1, :], attention_scores_without_scale[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\operatorname{FFN}(x)=\\max \\left(0, x W_1+b_1\\right) W_2+b_2$  \n",
    "Attention Is All You Needã®å¼(2)ã‚ˆã‚Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# MLPã§ã®å®Ÿè£…ã®éš›ã«å®Ÿè£…\n",
    "self.ff = nn.Sequential(\n",
    "    nn.Linear(d_model, d_ff),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(d_ff, d_model)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPT-2 Small\n",
    "vocab_size = 50257\n",
    "d_model = embedding_dim = 768 \n",
    "d_ff = d_model * 4\n",
    "n_head = 12 \n",
    "n_layer = 12\n",
    "d_k = int(d_model / n_head)\n",
    "d_v = int(d_model / n_head)\n",
    "\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "Wq = torch.randn((d_k, embedding_dim))\n",
    "Wk = torch.randn((d_k, embedding_dim))\n",
    "\n",
    "Wv = torch.randn((d_v, embedding_dim))\n",
    "Wo = torch.randn((d_model, d_v * n_head))\n",
    "\n",
    "Wff_1 = torch.randn((d_ff, d_model))\n",
    "Wff_2 = torch.randn((d_model, d_ff))\n",
    "Wy = torch.randn((vocab_size, d_model))\n",
    "\n",
    "params_embedding = vocab_size * embedding_dim\n",
    "print('embddingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_embedding)\n",
    "\n",
    "params_positional_embedding = d_model * 1024\n",
    "print('positional embeddingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_positional_embedding)\n",
    "\n",
    "params_self_attention = (d_model * d_k * n_head * 2 + d_model * d_v * n_head + d_model * d_v * n_head) * n_layer\n",
    "print('self-attentionã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_self_attention)\n",
    "\n",
    "params_ff = (d_ff * d_model * 2) * n_layer\n",
    "print('feed-forwardã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_ff)\n",
    "\n",
    "total_params = params_positional_embedding + params_embedding + params_self_attention + params_ff # + params_unembedding weight tied\n",
    "print('åˆè¨ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', total_params)\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®å†…è¨³å‰²åˆ\n",
    "print('embeddingã®å‰²åˆ', params_embedding / total_params)\n",
    "print('positional embeddingã®å‰²åˆ', params_positional_embedding / total_params)\n",
    "print('self-attentionã®å‰²åˆ', params_self_attention / total_params)\n",
    "print('feed-forwardã®å‰²åˆ', params_ff / total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1å±¤ã®FFã¨Attentionã®ã¿ã®ãƒ¢ãƒ‡ãƒ«\n",
    "vocab_size = len(vocab)\n",
    "d_model = embedding_dim = 3\n",
    "d_ff = d_model * 4\n",
    "n_head = 1 \n",
    "n_layer = 1 \n",
    "d_k = int(d_model / n_head)\n",
    "d_v = int(d_model / n_head)\n",
    "\n",
    "word_embedding_table = torch.randn((vocab_size, embedding_dim))\n",
    "\n",
    "Wq = torch.randn((d_k, embedding_dim))\n",
    "Wk = torch.randn((d_k, embedding_dim))\n",
    "\n",
    "Wv = torch.randn((d_v, embedding_dim))\n",
    "Wo = torch.randn((d_model, d_v * n_head))\n",
    "\n",
    "Wff_1 = torch.randn((d_ff, d_model))\n",
    "Wff_2 = torch.randn((d_model, d_ff))\n",
    "Wy = torch.randn((vocab_size, d_model))\n",
    "\n",
    "params_embedding = vocab_size * embedding_dim\n",
    "print('embddingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_embedding)\n",
    "\n",
    "params_positional_embedding = d_model * 1024\n",
    "print('positional embeddingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_positional_embedding)\n",
    "\n",
    "params_self_attention = (d_model * d_k * n_head * 2 + d_model * d_v * n_head + d_model * d_v * n_head) * n_layer\n",
    "print('self-attentionã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_self_attention)\n",
    "\n",
    "params_ff = (d_ff * d_model * 2) * n_layer\n",
    "print('feed-forwardã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', params_ff)\n",
    "\n",
    "total_params = params_positional_embedding + params_embedding + params_self_attention + params_ff # + params_unembedding weight tied\n",
    "print('åˆè¨ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°', total_params)\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®å†…è¨³å‰²åˆ\n",
    "print('embeddingã®å‰²åˆ', params_embedding / total_params)\n",
    "print('positional embeddingã®å‰²åˆ', params_positional_embedding / total_params)\n",
    "print('self-attentionã®å‰²åˆ', params_self_attention / total_params)\n",
    "print('feed-forwardã®å‰²åˆ', params_ff / total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸¦åˆ—åŒ–ã•ã‚Œã¦ã„ãªã„Self-Attentionå±¤ã€FeedForwardå±¤ã®è¨ˆç®—\n",
    "quries = []\n",
    "keys = []\n",
    "values = []\n",
    "attention_scores = []\n",
    "attention_outputs = []\n",
    "input_history = []\n",
    "\n",
    "# Self-Attentionã®è¨ˆç®—è¤‡é›‘åº¦ len(model_input.tolist()) * len(model_input.tolist()) * d_model = T * T * d\n",
    "for t in range(len(model_input.tolist())):\n",
    "    idx = model_input.tolist()[t]\n",
    "    input_history.append(idx_to_ch[idx])\n",
    "    x = torch.LongTensor([idx])\n",
    "    x = word_embedding_table[x].T # + positional_encodings\n",
    "    \n",
    "    # single-head, multi-headã«ãªã‚‹ã¨è¤‡æ•°ã®è¦³ç‚¹ã§query, key, valueã®ç™ºè¡Œã‚’ã™ã‚‹\n",
    "    query_t = torch.matmul(Wq, x) # ã€‡ã€‡æ¢ã—ã¦ã¾ã™ï¼ ä¾‹: token_0: è‡ªåˆ†ä¸»èªã§ã™ã€ç›®çš„èªã¨ã‹åŠ©è©ã¨ã‹å‹•è©ã¨ã‹æ¢ã—ã¦ã¾ã™ï¼\n",
    "\n",
    "    key_t = torch.matmul(Wk, x) # ã€‡ã€‡æŒã£ã¦ã¾ã™ï¼ ä¾‹: token_0: è‡ªåˆ†ä¸»èªã§ã™ï¼ token_1: è‡ªåˆ†å‹•è©ã§ã™!\n",
    "    value_t = torch.matmul(Wv, x) # ä¸­èº«ã®è©³ç´°ã§ã™ï¼ ä¾‹: token_0: ã€Œæ‹™è€…ã€ã§ã™ã€çã—ã„1äººç§°ã§ã™ã€ãŠä¾ã•ã‚“ã¨ã‹ãŒä½¿ã£ãŸã‚Šã—ã¾ã™ token_1: ã€Œé£Ÿã¹ã‚‹ã€ã§ã™ã€é£Ÿã¹ç‰©ã‚’å£ã«å…¥ã‚Œã‚‹è¡Œç‚ºã§ã™\n",
    "    \n",
    "    # cross attentionã®å ´åˆã¯key, valueã¯\n",
    "    # key_t = torch.matmul(Wk, another_modality_x)\n",
    "    # value_t = torch.matmul(Wv, another_modality_x)ã®ã‚ˆã†ã«ãªã‚‹\n",
    "\n",
    "    quries.append(query_t)\n",
    "    keys.append(key_t)\n",
    "    values.append(value_t)\n",
    "    # Day2ã®æ¼”ç¿’ã§ã¯æ–‡ç« å˜ä½ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ãŸqueryã¨keyã®å†…ç©(é¡ä¼¼åº¦)ã‚’ã‚‚ã¨ã«Retrievalã‚’è¡Œã£ãŸ\n",
    "    # è¨ˆç®—è¤‡é›‘åº¦ T * dã€ç³»åˆ—é•·ã«ã‚ˆã£ã¦å¤‰åŒ–\n",
    "    # é ãé›¢ã‚ŒãŸéå»ã®æ–‡è„ˆã‚’è€ƒæ…®ã§ãã‚‹ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å¿œã˜ã¦ç›¸äº’ä½œç”¨ã—ã€ãã®çµæœãŒå¾Œã®æ·±ã„å±¤ã§ã‚‚åæ˜ ã•ã‚Œã‚‹\n",
    "    attention_score = torch.matmul(query_t.T, torch.stack(keys)) / torch.sqrt(torch.tensor(d_k))\n",
    "    attention_score = torch.softmax(attention_score, dim=0)\n",
    "    attention_scores.append(attention_score.squeeze())\n",
    "    self_attention_output = 0\n",
    "    for i in range(len(attention_score)):\n",
    "        # queryã€keyã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœãŒvalueã«åæ˜ ã•ã‚Œã‚‹(attention scoreã§é‡ã¿ä»˜ã‘)\n",
    "        self_attention_output += attention_score[i] * values[i]\n",
    "    attention_outputs.append(self_attention_output)\n",
    "\n",
    "    # FeedForwardã®è¨ˆç®—\n",
    "    ff_output = torch.matmul(Wff_2, torch.relu(torch.matmul(Wff_1, self_attention_output)))\n",
    "    # æ¬¡å˜èªäºˆæ¸¬ã€ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã®ãŸã‚ã®åˆ†é¡å™¨ã‚’ä½œã‚‹ã‚ˆã†ãªå ´åˆã«ã¯æ–°ã—ãWyã‚’ç”¨æ„ã—ã¦å­¦ç¿’ã™ã‚‹\n",
    "    logits = torch.matmul(Wy, ff_output)\n",
    "    output_dist = [float('{:.3f}'.format(output)) for output in torch.softmax(logits, dim=0).squeeze().tolist()]\n",
    "    \n",
    "    print(f'P({idx_to_ch[model_target.tolist()[t]]} | {\", \".join(input_history)}) = {torch.softmax(logits, dim=0).squeeze().tolist()[model_target.tolist()[t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[t]] = 1\n",
    "    print(f'{output_dist} <-----å­¦ç¿’ã«ã‚ˆã£ã¦è¿‘ã¥ã‘ã‚‹-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentionã®å¯è¦–åŒ–\n",
    "attention_map = torch.zeros((len(model_input.tolist()), len(model_input.tolist())))\n",
    "for t in range(len(model_input.tolist())):\n",
    "    attention_map[t][:t+1] = attention_scores[t]\n",
    "    print('token', input_history)\n",
    "    print(input_history[t], attention_map[t].tolist())\n",
    "    print('-----' * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœªæ¥ã®æƒ…å ±ã‚’ç”¨ã„ãªã„ã‚ˆã†ã«Causal Attentionã‚’ç”¨ã„ã¦ä¸¦åˆ—åŒ–(è¡Œåˆ—ã®è¨ˆç®—ã«ã™ã‚‹)\n",
    "input_ids = model_input.tolist()\n",
    "x = torch.LongTensor([input_ids])\n",
    "x = word_embedding_table[x] # + positional_encodings\n",
    "# print(x.shape) # (1, T, embedding_dim)\n",
    "\n",
    "queries = torch.matmul(x.squeeze(), Wq.T) # (T, d_k)\n",
    "keys = torch.matmul(x.squeeze(), Wk.T) # (T, d_k)\n",
    "values = torch.matmul(x.squeeze(), Wv.T) # (T, d_v)\n",
    "# print(queries.shape, keys.shape, values.shape)\n",
    "\n",
    "attention_scores = torch.matmul(queries, keys.T) / torch.sqrt(torch.tensor(d_k)) # (T, T)\n",
    "# causal attentionã€æœªæ¥ã®æƒ…å ±ã‚’ç”¨ã„ãªã„ã‚ˆã†ã«ã™ã‚‹\n",
    "attention_mask = torch.tril(torch.ones((len(model_input.tolist()), len(model_input.tolist())))) # (T, T)\n",
    "attention_scores = attention_scores.masked_fill(attention_mask==0, float('-inf'))\n",
    "\n",
    "attention_scores = torch.softmax(attention_scores, dim=1) # (T, T)\n",
    "attention_outputs = torch.matmul(attention_scores, values) # (T, d_v)\n",
    "\n",
    "# FeedForwardã®è¨ˆç®—\n",
    "ff_output = torch.matmul(torch.relu(torch.matmul(attention_outputs, Wff_1.T)), Wff_2.T) # (T, d_model)\n",
    "# print(ff_output.shape)\n",
    "# æ¬¡å˜èªäºˆæ¸¬\n",
    "logits = torch.matmul(ff_output, Wy.T) # (T, vocab_size)\n",
    "output_dists = torch.softmax(logits, dim=1) # (T, vocab_size)\n",
    "\n",
    "for step_t, output_dist in enumerate(output_dists):\n",
    "    print(f'P({idx_to_ch[model_target.tolist()[step_t]]} | {\", \".join(input_history[:step_t + 1])}) = {output_dist[model_target.tolist()[step_t]]:.3f}')\n",
    "    model_target_onehot = torch.zeros((1, vocab_size))\n",
    "    model_target_onehot[0, model_target.tolist()[step_t]] = 1\n",
    "    print(f'{[float(\"{:.3f}\".format(output)) for output in output_dist.tolist()]} <-----å­¦ç¿’ã«ã‚ˆã£ã¦è¿‘ã¥ã‘ã‚‹-----> {model_target_onehot.tolist()[0]}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = torch.matmul(queries, keys.T) / torch.sqrt(torch.tensor(d_k)) # (T, T)\n",
    "print('causal attentionå‰')\n",
    "print(attention_scores)\n",
    "# causal attentionã®å ´åˆã¯attention_maskã‚’ç”¨ã„ã¦æœªæ¥ã®æƒ…å ±ã‚’ãƒã‚¹ã‚¯ã™ã‚‹\n",
    "attention_mask = torch.tril(torch.ones((len(model_input.tolist()), len(model_input.tolist())))) # (T, T)\n",
    "# ä¸‹ä¸‰è§’è¡Œåˆ—ã§æœªæ¥ã®æƒ…å ±ã‚’ãƒã‚¹ã‚¯\n",
    "print(attention_mask)\n",
    "# æœªæ¥ã®æƒ…å ±ã¯scoreãŒ0ã«ãªã‚‹ã‚ˆã†ã«-infã‚’ä»£å…¥(softmaxã§0ã«ãªã‚‹)\n",
    "attention_scores = attention_scores.masked_fill(attention_mask==0, float('-inf'))\n",
    "print('causal attentionå¾Œ')\n",
    "print(attention_scores)\n",
    "attention_scores = torch.softmax(attention_scores, dim=1) # (T, T)\n",
    "print('softmaxå¾Œ')\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model, d_head):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.query = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.back_to_d_model = nn.Linear(d_head, d_model)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, d_model)\n",
    "        B, T, d_model = x.size()\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        qk_dot_product = q @ k.transpose(-2, -1) / (d_head ** 0.5)\n",
    "        mask = torch.tril(torch.ones((T, T))).to(qk_dot_product.device)\n",
    "        qk_dot_product = qk_dot_product.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_score = torch.softmax(qk_dot_product, dim=-1)\n",
    "        out = attention_score @ v\n",
    "        out = self.back_to_d_model(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionLM(nn.Module):\n",
    "    def __init__(self, vocab_size, sequence_length, d_model, d_head):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(sequence_length, d_model)\n",
    "        self.head = Head(d_model, d_head)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "        print('number of parameters:', sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        B, T = token_indexes.size()\n",
    "        token_embed = self.embed(token_indexes)\n",
    "        pos_embed = self.pos_embed(torch.arange(T).to(token_embed.device))\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.head(x)\n",
    "        logits = self.unembed(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length),\n",
    "            reduction='none'\n",
    "            )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "d_model = 4\n",
    "d_head = 4\n",
    "sequence_length = 512\n",
    "attention_lm = AttentionLM(vocab_size, sequence_length, d_model, d_head).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_data_tokens = []\n",
    "mini_train_data_file_num = 1000\n",
    "mini_train_data_text = \"\"\n",
    "file_count = 0\n",
    "\n",
    "with gzip.open('train_9.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # å„è¡Œã‚’JSONã¨ã—ã¦èª­ã¿è¾¼ã‚€\n",
    "        data = json.loads(line)\n",
    "        mini_train_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == mini_train_data_file_num:\n",
    "            break\n",
    "\n",
    "val_data_tokens = []\n",
    "val_data_file_num = 1\n",
    "val_data_text = \"\"\n",
    "file_count = 0\n",
    "with gzip.open('validation_0.jsonl.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # å„è¡Œã‚’JSONã¨ã—ã¦èª­ã¿è¾¼ã‚€\n",
    "        data = json.loads(line)\n",
    "        val_data_tokens += tokenizer.encode(data['text'], eot=True)\n",
    "        file_count += 1\n",
    "        if file_count == val_data_file_num:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’\n",
    "optimizer = torch.optim.AdamW(attention_lm.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "training_tokens = 0\n",
    "print('before training')\n",
    "val_loss = 0\n",
    "for i in range(0, len(val_data_tokens), sequence_length+1):\n",
    "    batch_tokens = val_data_tokens[i:i+sequence_length+1]\n",
    "    input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "    target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = attention_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "    val_loss += loss.sum().item()\n",
    "val_loss = val_loss / len(val_data_tokens)\n",
    "print(f'val_loss: {val_loss}')\n",
    "print('start training')\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(mini_train_data_tokens), sequence_length+1):\n",
    "        batch_tokens = mini_train_data_tokens[i:i+sequence_length+1]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        loss = attention_lm.loss(input_token_indexes, target_token_indexes)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_tokens += len(batch_tokens)\n",
    "        if training_tokens % ((sequence_length+1)*1000) == 0:\n",
    "            print(f'epoch: {epoch}, loss: {loss.item()}, training_tokens: {training_tokens}')\n",
    "    val_loss = 0\n",
    "    for i in range(0, len(val_data_tokens), sequence_length+1):\n",
    "        batch_tokens = val_data_tokens[i:i+sequence_length+1]\n",
    "        input_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, :-1].to(device)\n",
    "        target_token_indexes = torch.tensor(batch_tokens).unsqueeze(0)[:, 1:].to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = attention_lm.loss_per_token(input_token_indexes, target_token_indexes)\n",
    "        val_loss += loss.sum().item()\n",
    "    val_loss = val_loss / len(val_data_tokens)\n",
    "    print(f'epoch: {epoch}, val_loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "input_tokens =  mini_train_data_tokens[:context_length]\n",
    "target_tokens = mini_train_data_tokens[1:context_length+1]\n",
    "input_token_indexes = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "target_token_indexes = torch.tensor(target_tokens).unsqueeze(0).to(device)\n",
    "attention_lm.loss(input_token_indexes, target_token_indexes).item()\n",
    "\n",
    "# bi-gram(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° 16516096)ã®æ€§èƒ½: 5.082467079162598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'ã€å‹ã¤ã‹'\n",
    "context_token_indexes = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "generated_tokens = attention_lm.generate(context_token_indexes, max_new_tokens=20)\n",
    "for token in generated_tokens[0]:\n",
    "    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šè¨˜ã®å®Ÿè£…ã«åŠ ãˆã€å­¦ç¿’ã®å®‰å®šåŒ–ã®ãŸã‚ã«Residual Connectionã¨Layer Normalizationã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§Transformerã®blockã‚’å®Ÿè£…ã§ãã¾ã™ã€‚\n",
    "ä»¥ä¸‹ã«GPT-2ã®å®Ÿè£…ã¨å­¦ç¿’ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2ã®å®Ÿè£… code from https://github.com/karpathy/makemore/tree/master\n",
    "# https://github.com/karpathy/makemore/blob/master/makemore.py\n",
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Andrej Karpathy\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Transformer Language Model (*exactly* as used in GPT-2)\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual connection\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # residual connection\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ•°: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¼”ç¿’3: Transformerã®ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’å¤‰æ›´ã—ã¦æ€§èƒ½å‘ä¸Šã‚’è©¦ã¿ã‚‹ã€GPT-2æ§‹é€ ã§å­¦ç¿’ã•ã›ã¦ã¿ã‚‹"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒ\n",
    "- [Andrej Karpathy(å…ƒTeslaã®AIãƒãƒ¼ãƒ ã®ãƒªãƒ¼ãƒ€ãƒ¼ã€OpenAIã®å…±åŒå‰µæ¥­è€…)ã«ã‚ˆã‚‹GPT-2å®Ÿè£…ã¾ã§ã®è¬›ç¾©å‹•ç”»](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
    "- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)  \n",
    "- [CS224N: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "    - https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture08-transformers.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
