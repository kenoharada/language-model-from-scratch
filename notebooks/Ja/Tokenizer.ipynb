{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character base tokenizer \n",
    "常用漢字、ひらがな、カタカナ、アルファベット、記号を1文字単位でtokenizeするtokenizerを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.bunka.go.jp/kokugo_nihongo/sisaku/joho/joho/kijun/naikaku/kanji/joyokanjisakuin/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html PUBLIC \"-//W3C//Dtd XHTML 1.0 Transitional//EN\" \"http://www.w3.org/tr/xhtml1/Dtd/xhtml1-transitional.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"ja\"><!-- InstanceBegin template=\"/Templates/bottom.dwt\" codeOutsideHTMLIsLocked=\"false\" -->\\n<head>\\n<meta http-equiv=\"Content-'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTMLファイルの読み込み\n",
    "with open(\"index.html\", encoding=\"cp932\") as f:\n",
    "    html = f.read()\n",
    "html[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table id=\"urlist\" class=\"display\" border=\"1\" cellspacing=\"0\"\n",
       "summary=\"kanji\" cellpadding=\"2\" width=\"100%\">\n",
       "  <thead>\n",
       "  <tr>\n",
       "    <th bgcolor=\"#cc9999\">漢字</th>\n",
       "    <th bgcolor=\"#cc9999\">音訓</th>\n",
       "    <th bgcolor=\"#cc9999\">例</th>\n",
       "    <th bgcolor=\"#cc9999\">備考</th>\n",
       "  </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "\n",
       "<tr><td><font size=\"7\">亜</font><font size=\"6\">（亞）</font></td><td>ア</td><td>亜流，亜麻，亜熱帯</td><td>　</td></tr>\n",
       "<tr><td><font size=\"7\">哀</font></td><td>アイ　<br />あわれ　<br />あわれむ</td><td>哀愁，哀願，悲哀<br />哀れ，哀れな話，哀れがる<br />哀れむ，哀れみ</td><td>　</td></tr>\n",
       "<tr><td><font size=\"7\">挨</font></td><td>アイ</td><td>挨拶</td><td>　</td></tr>\n",
       "<tr><td><font size=\"7\">愛</font></td><td>アイ</td><td>愛情，愛読，恋愛</td><td>愛媛（えひめ）県</td></tr>  \n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "table_sample = '''\n",
    "<table id=\"urlist\" class=\"display\" border=\"1\" cellspacing=\"0\"\n",
    "summary=\"kanji\" cellpadding=\"2\" width=\"100%\">\n",
    "  <thead>\n",
    "  <tr>\n",
    "    <th bgcolor=\"#cc9999\">漢字</th>\n",
    "    <th bgcolor=\"#cc9999\">音訓</th>\n",
    "    <th bgcolor=\"#cc9999\">例</th>\n",
    "    <th bgcolor=\"#cc9999\">備考</th>\n",
    "  </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "\n",
    "<tr><td><font size=\"7\">亜</font><font size=\"6\">（亞）</font></td><td>ア</td><td>亜流，亜麻，亜熱帯</td><td>　</td></tr>\n",
    "<tr><td><font size=\"7\">哀</font></td><td>アイ　<br />あわれ　<br />あわれむ</td><td>哀愁，哀願，悲哀<br />哀れ，哀れな話，哀れがる<br />哀れむ，哀れみ</td><td>　</td></tr>\n",
    "<tr><td><font size=\"7\">挨</font></td><td>アイ</td><td>挨拶</td><td>　</td></tr>\n",
    "<tr><td><font size=\"7\">愛</font></td><td>アイ</td><td>愛情，愛読，恋愛</td><td>愛媛（えひめ）県</td></tr>  \n",
    "  </tbody>\n",
    "</table>\n",
    "'''\n",
    "HTML(table_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2136\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "joyo_kanji_info = dict()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "table = soup.find(\"table\", id=\"urlist\")\n",
    "\n",
    "for row in table.find_all(\"tr\"):\n",
    "    if row.th:\n",
    "        continue\n",
    "    cols = row.find_all(\"td\")\n",
    "    kanji = cols[0].find(\"font\").text\n",
    "    on_kun = cols[1].text\n",
    "    examples = cols[2].text\n",
    "    note = cols[3].text\n",
    "    joyo_kanji_info[kanji] = {\"音訓\": on_kun, \"例\": examples, \"備考\": note}\n",
    "print(len(joyo_kanji_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'亜哀挨愛曖悪握圧扱宛'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(list(joyo_kanji_info.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "0123456789\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "' \\t\\n\\r\\x0b\\x0c'\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# アルファベット\n",
    "print(string.ascii_letters)\n",
    "# 数字\n",
    "print(string.digits)\n",
    "# 句読点\n",
    "print(string.punctuation)\n",
    "# 改行など空白文字\n",
    "print(repr(string.whitespace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろゎわゐゑをんゔゕゖ゗゘゙゚゛゜ゝゞ\n",
      "ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヰヱヲンヴヵヶヷヸヹヺ・ーヽヾ\n",
      "　、。〃〄々〆〇〈〉《》「」『』【】〒〓〔〕〖〗〘〙〚〛〜〝〞〟〠〡〢〣〤〥〦〧〨〩〪〭〮〯〫〬〰〱〲〳〴〵〶〷〸〹〺〻〼〽〾\n",
      "＀！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ［＼］＾＿｀ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ｛｜｝～｟｠｡｢｣､･ｦｧｨｩｪｫｬｭｮｯｰｱｲｳｴｵｶｷｸｹｺｻｼｽｾｿﾀﾁﾂﾃﾄﾅﾆﾇﾈﾉﾊﾋﾌﾍﾎﾏﾐﾑﾒﾓﾔﾕﾖﾗﾘﾙﾚﾛﾜﾝﾞﾟﾠﾡﾢﾣﾤﾥﾦﾧﾨﾩﾪﾫﾬﾭﾮﾯﾰﾱﾲﾳﾴﾵﾶﾷﾸﾹﾺﾻﾼﾽﾾ﾿￀￁ￂￃￄￅￆￇ￈￉ￊￋￌￍￎￏ￐￑ￒￓￔￕￖￗ￘￙ￚￛￜ￝￞￟￠￡￢￣￤￥￦￧￨￩￪￫￬￭￮\n"
     ]
    }
   ],
   "source": [
    "# https://www.unicode.org/charts/nameslist/\n",
    "# ひらがな\n",
    "start = 0x3041\n",
    "end = 0x309F\n",
    "hiragana = \"\".join(chr(i) for i in range(start, end))\n",
    "print(hiragana)\n",
    "\n",
    "# カタカナ\n",
    "start = 0x30A1\n",
    "end = 0x30FF\n",
    "katakana = \"\".join(chr(i) for i in range(start, end))\n",
    "print(katakana)\n",
    "\n",
    "# CJK Symbols and Punctuation\n",
    "start = 0x3000\n",
    "end = 0x303F\n",
    "cjk_symbols_punctuation = \"\".join(chr(i) for i in range(start, end))\n",
    "print(cjk_symbols_punctuation)\n",
    "\n",
    "# Halfwidth and Fullwidth Forms\n",
    "start = 0xFF00\n",
    "end = 0xFFEF\n",
    "halfwidth_fullwidth_forms = \"\".join(chr(i) for i in range(start, end))\n",
    "print(halfwidth_fullwidth_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.unknown_idx = -1\n",
    "        if vocab is None:\n",
    "            vocab = {}\n",
    "        self.vocab = vocab\n",
    "        vocab[\"<UNK>\"] = self.unknown_idx\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.vocab.get(token, self.unknown_idx) for token in text]\n",
    "    \n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        return \"\".join([inv_vocab[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict()\n",
    "entire_text = \"\"\n",
    "entire_text += string.ascii_letters\n",
    "entire_text += string.digits\n",
    "entire_text += string.punctuation\n",
    "entire_text += string.whitespace\n",
    "entire_text += hiragana\n",
    "entire_text += katakana\n",
    "entire_text += cjk_symbols_punctuation\n",
    "entire_text += halfwidth_fullwidth_forms\n",
    "for kanji in joyo_kanji_info.keys():\n",
    "    entire_text += kanji\n",
    "entire_text = \"\".join(sorted(list(set(entire_text))))\n",
    "for i, char in enumerate(entire_text):\n",
    "    if char not in vocab:\n",
    "        vocab[char] = i\n",
    "tokenizer = CharTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[489, 208, 655, 560, 209, 626, 1618, 201, 187, 102]\n",
      "僕の名前は原田です。\n"
     ]
    }
   ],
   "source": [
    "text = \"僕の名前は原田です。\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 489, 208, 655, 560, 209, 626, 1618, 201, 187, 102, -1]\n",
      "<UNK>僕の名前は原田です。<UNK>\n"
     ]
    }
   ],
   "source": [
    "text = '亞僕の名前は原田です。😄'\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharUTF8Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        if vocab is None:\n",
    "            vocab = {}\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        vocab_size = len(vocab) * 1000\n",
    "        for i in range(256):\n",
    "            vocab[f'<utf8_{i}>'] = vocab_size + i\n",
    "\n",
    "    def encode(self, text):\n",
    "        result = []\n",
    "        for char in text:\n",
    "            if char not in self.vocab:\n",
    "                utf_8_num = list(char.encode(\"utf-8\"))\n",
    "                for num in utf_8_num:\n",
    "                    result.append(self.vocab[f'<utf8_{num}>'])\n",
    "            else:\n",
    "                result.append(self.vocab[char])\n",
    "        return result\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        decoded_with_utf_token = [inv_vocab[token] for token in tokens]\n",
    "        decoded_postprocess_utf = []\n",
    "        utf_tokens = []\n",
    "        for token in decoded_with_utf_token:\n",
    "            if token.startswith(\"<utf8_\"):\n",
    "                utf_num = int(token.replace(\"<utf8_\", \"\").replace(\">\", \"\"))\n",
    "                utf_tokens.append(utf_num)\n",
    "            else:\n",
    "                if utf_tokens:\n",
    "                    decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "                    utf_tokens = []\n",
    "                decoded_postprocess_utf.append(token)\n",
    "        if utf_tokens:\n",
    "            decoded_postprocess_utf.append(bytes(utf_tokens).decode(\"utf-8\"))\n",
    "        return \"\".join(decoded_with_utf_token), \"\".join(decoded_postprocess_utf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_utf8_vocab = dict()\n",
    "entire_text = \"\"\n",
    "entire_text += string.ascii_letters\n",
    "entire_text += string.digits\n",
    "entire_text += string.punctuation\n",
    "entire_text += string.whitespace\n",
    "entire_text += hiragana\n",
    "entire_text += katakana\n",
    "entire_text += cjk_symbols_punctuation\n",
    "entire_text += halfwidth_fullwidth_forms\n",
    "for kanji in joyo_kanji_info.keys():\n",
    "    entire_text += kanji\n",
    "entire_text = \"\".join(sorted(list(set(entire_text))))\n",
    "for i, char in enumerate(entire_text):\n",
    "    if char not in char_utf8_vocab:\n",
    "        char_utf8_vocab[char] = i\n",
    "tokenizer = CharUTF8Tokenizer(char_utf8_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2726228, 2726186, 2726158, 489, 208, 655, 560, 209, 626, 1618, 201, 187, 102, 2726240, 2726159, 2726152, 2726132]\n",
      "<utf8_228><utf8_186><utf8_158>僕の名前は原田です。<utf8_240><utf8_159><utf8_152><utf8_132>\n",
      "亞僕の名前は原田です。😄\n"
     ]
    }
   ],
   "source": [
    "text = '亞僕の名前は原田です。😄'\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded)\n",
    "decoded_with_utf, decoded = tokenizer.decode(encoded)\n",
    "print(decoded_with_utf)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 74, 81, 81, 84]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# with open(\"char_utf8_vocab.json\", \"w\") as f:\n",
    "#     json.dump(char_utf8_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# load\n",
    "with open(\"char_utf8_vocab.json\", \"r\") as f:\n",
    "    char_utf8_vocab = json.load(f)\n",
    "tokenizer = CharUTF8Tokenizer(char_utf8_vocab)\n",
    "tokenizer.encode('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0b1100001']\n",
      "['0b11100011', '0b10000001', '0b10000010']\n"
     ]
    }
   ],
   "source": [
    "print([bin(utf_8) for utf_8 in list('a'.encode(\"utf-8\"))])\n",
    "print([bin(utf_8) for utf_8 in list('あ'.encode(\"utf-8\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.mext.go.jp/a_menu/shotou/new-cs/__icsFiles/afieldfile/2017/05/15/1385768.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install camelot-py[cv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
